---
title: "Dissertation Analysis"
author: "Jamiahus Walton"
bibliography: bibliography.bib
output:
  html_document: default
editor_options:
  chunk_output_type: inline
csl: apa.csl
Date: '2019-02-01'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Tips for formatting in RMarkdown
# Link: https://monashbioinformaticsplatform.github.io/2017-11-16-open-science-training/topics/rmarkdown.html

# Equations
# Link: https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html

# Create awesome HTML table with knitr::kableand kableExtra
# LinkL https://haozhu233.github.io/kableExtra/awesome_table_in_html.html

# Examples of how to use the ggrepel package
# Link: https://cran.r-project.org/web/packages/ggrepel/vignettes/ggrepel.html#examples

# Packages for data analysis
library(tidyverse)
library(lme4)
library(lmerTest)
library(emmeans)
library(svMisc)
library(MuMIn)
library(modelr)
library(sjstats)
library(robustlmm)
library(ggrepel)
library(knitr)
library(kableExtra)

# Functions used in documents ----
remove_measures_with_given_value <- function(data_set, col_name, value){
  rows_to_move <- which(as.vector(data_set[,col_name]) == value) 
  
  return(data_set[-rows_to_move,])
}

# Factor columns that need it.
re_factor_columns <- function(userData, columnNames){
  factorData <- userData
  for(column in columnNames){
    print(column)
    factorData[,column] <- factor(factorData[,column])
  }
  return(factorData)
}

# Model the data for the team level analysis ----
model_data_Target_Session <- function(df, dependent, model.type, is.team, is.robust){
  
  if(is.team){
    if(model.type == "null" && !is.robust){
      lmer(data = df, as.formula(paste(dependent,"~ 1 + (1|Team)")))
    } else if(model.type == "All"){
      lmer(data = df, as.formula(paste(dependent,"~ Target * SessionOrder + (1|Team)")))
    } else if(model.type == "NoInteraction" && !is.robust){
      lmer(data = df, as.formula(paste(dependent,"~ Target + SessionOrder + (1|Team)")))
    } else if(model.type == "NoInteraction_NoTarget" && !is.robust){
      lmer(data = df, as.formula(paste(dependent,"~ SessionOrder + (1|Team)")))
    } else if(model.type == "NoInteraction_NoSession" && !is.robust){
      lmer(data = df, as.formula(paste(dependent,"~ Target + (1|Team)")))
    } else if(model.type == "null" && is.robust){
      rlmer(data = df, as.formula(paste(dependent,"~ 1 + (1|Team)")))
    } else if(model.type == "All" && is.robust){
      rlmer(data = df, as.formula(paste(dependent,"~ Target * SessionOrder + (1|Team)")))
    } else if(model.type == "NoInteraction" && is.robust){
      rlmer(data = df, as.formula(paste(dependent,"~ Target + SessionOrder + (1|Team)")))
    } else if(model.type == "NoInteraction_NoTarget" && is.robust){
      rlmer(data = df, as.formula(paste(dependent,"~ SessionOrder + (1|Team)")))
    } else if(model.type == "NoInteraction_NoSession" && is.robust){
      rlmer(data = df, as.formula(paste(dependent,"~ Target + (1|Team)")))
    } else{
      stop("Model.type not supported")
    }
  } else {
    # Run this code if individual level model
    if(model.type == "null" && !is.robust){
      lmer(data = df, as.formula(paste(dependent,"~ 1 + (1|Team) + (1| Player_ID)")))
    } else if(model.type == "All" && !is.robust){
      lmer(data = df, as.formula(paste(dependent,"~ Target * SessionOrder + (1|Team) + (1| Player_ID)")))
    } else if(model.type == "NoInteraction" && !is.robust){
      lmer(data = df, as.formula(paste(dependent,"~ Target + SessionOrder + (1|Team) + (1| Player_ID)")))
    } else if(model.type == "NoInteraction_NoTarget" && !is.robust){
      lmer(data = df, as.formula(paste(dependent,"~ SessionOrder + (1|Team) + (1| Player_ID)")))
    } else if(model.type == "NoInteraction_NoSession"){
      lmer(data = df, as.formula(paste(dependent,"~ Target + (1|Team) + (1| Player_ID)")))
    } else if(model.type == "null" && is.robust){
      rlmer(data = df, as.formula(paste(dependent,"~ 1 + (1|Team) + (1| Player_ID)")))
    } else if(model.type == "All" && is.robust){
      rlmer(data = df, as.formula(paste(dependent,"~ Target * SessionOrder + (1|Team) + (1| Player_ID)")))
    } else if(model.type == "NoInteraction" && is.robust){
      rlmer(data = df, as.formula(paste(dependent,"~ Target + SessionOrder + (1|Team) + (1| Player_ID)")))
    } else if(model.type == "NoInteraction_NoTarget" && is.robust){
      rlmer(data = df, as.formula(paste(dependent,"~ SessionOrder + (1|Team) + (1| Player_ID)")))
    } else if(model.type == "NoInteraction_NoSession" && is.robust){
      rlmer(data = df, as.formula(paste(dependent,"~ Target + (1|Team) + (1| Player_ID)")))
    } else{
      stop("Model.type not supported")
    }
  }
}

#Folder locations ----
figure_directory <- "C:\\Users\\jamia\\Box\\TMET2\\DATA TMET2\\Data_And_Calcuations\\Figures"
main_work_directory_name <- "C:\\Users\\jamia\\Box\\TMET2\\DATA TMET2\\Data_And_Calcuations\\Raw Data\\"

database_folder_name <- "Database"
file_name_output <- "team_player_aggragate_stats.csv"
folder_location_database <- paste(main_work_directory_name, database_folder_name, sep = "")
aggregate_folder_location <- paste(folder_location_database,"\\", file_name_output, sep = "") #This will combine the final file name and the desiered folder location

# Read aggregaate data ----
my_aggregate_data <- read.csv(file =  aggregate_folder_location)

clean_aggregate_data_stats <- remove_measures_with_given_value(data_set =  my_aggregate_data, col_name = "Condition", value = "A") # without none condition

# Re factor the columns
columns_to_refactor <- c("SessionOrder", 
                         "Team", 
                         "Player_ID", 
                         "Condition", 
                         "Dominate.Strategy", 
                         "Condition", 
                         "Target",
                         "Confident_team_comm_important_details_quickly")
clean_aggregate_data_stats <- re_factor_columns(clean_aggregate_data_stats, columns_to_refactor)

# What is the N for Teams ----
N_teams <- length(levels(factor(clean_aggregate_data_stats$Team)))

# What is the N for Inds ----
N_ind <- length(levels(factor(clean_aggregate_data_stats$Player_ID) ))

# Team data set ----
team_data <- clean_aggregate_data_stats %>%
  filter(Player == 1)

# Individual data set ----
ind_data <- clean_aggregate_data_stats

```

## Purpose

The purpose of this document is to record the data analysis for my Dissertation.

## Experimental Design

This experiment is a within-subject experimental design. The primary variable was the feedback condition (i.e., Target). There were four feedback conditions; None, Individual, Team, and Ind_Team Each team experienced each condition. The first condition for each group was the None condition. The remaining three conditions were counterbalanced. For example, the condition order for team 7 is the following: None, Ind, Team, Ind_Team. The condition order for group 8 is the following: None, Team, Individual, Ind_Team. 

## Data Description

I collected data from **`r N_ind`** participants that formed **`r N_teams`** teams. Below is a sample of the data for the main metrics

```{r main_metrics_table, echo=FALSE}
dt <- clean_aggregate_data_stats[1:5,c("TeamScore", 
                           "IndividualScore", 
                           "CI_team", 
                           "CI_ind", 
                           "II_team", 
                           "II_ind", 
                           "ERROR_team_unique", 
                           "ERROR_ind_unique", 
                           "ERROR_team_total",
                           "ERROR_ind_total")] 
dt %>%
  kable() %>%
  kable_styling()
```

## Research Question

This research attempts to answer the following question: *How will teams' performance change if given feedback that displays indicators based on individual performance, team performance, or both?*

## Exploratory Data Analysis

In this section, I seek to visually discover answers to the research question mentioned above.

### Q: Is there exciting variation among the *time remaining* metric that will help me understand the influence of the feedback condition?

#### Team

```{r hist_CountVSTimeRemaining}
dependent_response_team <- "timeRemaining_team"
y_label_team <- "Count"
x_label_team <- "Time Remaining"
title_response_team <- "Distribution of Time Remaining (Team)"
plot_name <- "Histogram_TimeRemaining_Team.png"
setwd(figure_directory)

plot_data_team <- team_data %>%
  select(dependent_response_team, Target)

ggplot(data = plot_data_team) + 
  geom_histogram(aes_string(x = dependent_response_team), bins = 30) +
  labs(title = title_response_team, x = x_label_team, y = y_label_team) +
  ggsave(filename = plot_name)
```

Overall, the distribution of the time remaining is skewed to the left. This skew tells me that most of the teams had around 50 seconds left, or less, at the end of each session. 

```{r}
dependent_response_team <- "timeRemaining_team"
y_label_team <- "Count"
x_label_team <- "Target"
title_response_team <- "Distribution of Time Remaining (Team)"
plot_name <- "Histogram_TimeRemaining_ByTarget_Team.png"
setwd(figure_directory)

plot_data_team <- team_data %>%
  select(dependent_response_team, Target)

ggplot(data = plot_data_team) + 
  geom_histogram(aes_string(x = dependent_response_team), bins = 25) +
  facet_grid(. ~ Target) +
  labs(title = title_response_team, x = x_label_team, y = y_label_team) +
  ggsave(filename = plot_name)

# Information about the N values in each condition
tableData <- team_data %>%
  group_by(Target) %>%
  summarise(N = length(.data[[dependent_response_team]]))

tableData %>%
  kable() %>%
  kable_styling()
```

The skewed to the left pattern continues when grouping the time remaining by the feedback conditions. Most of the teams seem to have about 50 seconds remaining when they completed the task 

__Reflection:__ The skewness in the time remaining data at the team level centers around 50 seconds or so. This skewness suggests that most of the teams had about the same amount of time remaining at the end of the session (50 or 40 seconds). In other words, there is no obvious pattern visually. 

#### Individual

```{r}
dependent_response_ind <- "timeRemaining_ind"
y_label_ind <- "Count"
x_label_ind <- "Time Remaining"
title_response_ind <- "Distribution of Time Remaining (Individual)"
plot_name <- "Histogram_TimeRemaining_Ind.png"
setwd(figure_directory)

plot_data_ind <- ind_data %>%
  select(dependent_response_ind, Target)

ggplot(data = plot_data_ind) + 
  geom_histogram(aes_string(x = dependent_response_ind), bins = 40) +
  labs(title = title_response_ind, x = x_label_ind, y = y_label_ind) +
  ggsave(filename = plot_name)
```

Overall, the time remaining at the individual level is skewed to the left. This skewness tells me that most of the participants had 40 to 50 seconds remaining at the end of the sessions. I also notice that there are a number of data points distributed from about 60 to 350. Are there similar characteristics among the teams that have more than 50 seconds remaining at the end of the sessions?

```{r}
dependent_response_ind <- "timeRemaining_ind"
y_label_ind <- "Count"
x_label_ind <- "Time Remaining"
title_response_ind <- "Distribution of Time Remaining (Individual)"
plot_name <- "Histogram_TimeRemaining_ByTarget_Ind.png"
setwd(figure_directory)

plot_data_ind <- ind_data %>%
  select(dependent_response_ind, Target)

ggplot(data = plot_data_ind) + 
  geom_histogram(aes_string(x = dependent_response_ind), bins = 30) +
  facet_grid(. ~ Target) +
  labs(title = title_response_ind, x = x_label_ind, y = y_label_ind) +
  ggsave(filename = plot_name)

# Information about the N values in each condition
tableData <- ind_data %>%
  group_by(Target) %>%
  summarise(N = length(.data[[dependent_response_ind]]))

tableData %>%
  kable() %>%
  kable_styling()
```

The distribution for each condition is similar to the overall data distribution. This similarity tells me that most individuals finished with about 50 seconds or so left at the end of each session. 

__Reflection:__ It seems as though we can say the high performing individuals finished with over 50 seconds or so left at the end of the session. Do high performing individuals have similar characteristics? How should we define a "high performing" individual? Overall, the distribution at the team level is similar to the distribution at the team level. 
  
  
#### Summary / Reflection

Overall, the data skewed to the left. According to the count at the individual and team level, there were fewer teams and fewer players that finished with more than 50 seconds or so left at the end of the session. We could say that the players and the teams that finished above this 50 seconds (or so) threshold were high performing / faster teams. A question that could be asked is do these "quick / high performing" teams have similar characteristics? 

There did not seem to be any obvious pattern when grouping the data by Feedback conditions. Maybe the feedback conditions influenced the amount of time remaining depending on their experience (i.e., session order)? 

### Q: Is there exciting variation among the *time remaining* that explains how the feedback conditions influence the *time remaining* when session order is taken into consideration?

#### Team

```{r}
dependent_response_team <- "timeRemaining_team"
y_label_team <- "Count"
x_label_team <- "Time Remaining"
title_response_team <- "Distribution of Time Remaining (Team)"
plot_name <- "Histogram_TimeRemaining_ByTarget_BySessionOrder_Team.png"
setwd(figure_directory)

plot_data_team <- team_data %>%
  select(dependent_response_team, Target, SessionOrder)

ggplot(data = plot_data_team) + 
  geom_histogram(aes_string(x = dependent_response_team), bins = 30) +
  labs(title = title_response_team, x = x_label_team, y = y_label_team) +
  facet_grid( SessionOrder ~ Target) + 
  ggsave(filename = plot_name)

# Information about the N values in each condition
tableData <- team_data %>%
  select(dependent_response_team, Target, SessionOrder) %>%
  group_by(Target, SessionOrder) %>%
  summarise(N = length(.data[[dependent_response_team]]))

tableData %>%
  kable() %>%
  kable_styling()
```

It looks like the distribution for time remaining for teams in the individual feedback condition did not change with more experience. The lack of change is unsual because I would expect the distrabution to move the right (i.e., increase with more experience). 
  
It looks like the distribution for time remaining for the Ind_Team condition seems to spread (or flatten) over time, which means that the teams were completing the task faster with more experience.
  
It looks like the distribution of the amount of time remaining at the end of the session in the team feedback condition starts to spread over time.

__Reflection:__ I would expect to see the distribution of time remaining to move to the right over time because the teams would will have more experience with the task and get better at the task. This pattern is not followed in the individual condition. The distribution of the time remaining in the individual condition does not seem to improve over time. The amount of time remaining seems to be consistent no matter what session order (i.e., 2,3, or 4) they are in. This consistency would suggest that giving individual performance metrics does not influence the amount of time remaining at the end of the session for a team. 

The distribution for the Ind_Team and Team condition seem to spread (i.e., flatten) over time instead of moving from let to right. To me, this suggest that there are some time that get better with time in these conditions and there are other teams that do no get better with time. 

#### Individual

```{r}
dependent_response_ind <- "timeRemaining_ind"
y_label_ind <- "Count"
x_label_ind <- "Time Remaining"
title_response_ind <- "Distribution of Time Remaining (Individual)"
plot_name <- "Histogram_TimeRemaining_ByTarget_BySessionOrder_Ind.png"
setwd(figure_directory)

plot_data_ind <- ind_data %>%
  select(dependent_response_ind, Target, SessionOrder)

ggplot(data = plot_data_ind) + 
  geom_histogram(aes_string(x = dependent_response_ind), bins = 30) +
  facet_grid(SessionOrder ~ Target) +
  labs(title = title_response_ind, x = x_label_ind, y = y_label_ind) +
  ggsave(filename = plot_name)

# Information about the N values in each condition
tableData <- ind_data %>%
  select(dependent_response_ind, Target, SessionOrder) %>%
  group_by(Target, SessionOrder) %>%
  summarise(N = length(.data[[dependent_response_ind]]))

tableData %>%
  kable() %>%
  kable_styling()
```

It looks like the distribution of the time remaining in the individual condition does not spread over time. This lack of spread tells me that the individuals were not getting faster over time in the individual feedback condition. It looks like the spread of the time remaining in the Ind_Team condition does spread over time. There is a more obvious spread from session 3 to session 4. It looks like the time remaining in the team condition does spread over time.

__Reflection:__ I am getting the sense that the individual condition did not influence how quickly the participants completed the session. In other words, giving the individual feedback on performance metrics did not encourage the participants to move faster through the session. However, it looks like the distrabution of the time remaining flattens when groups are given Team or Ind_Team feedback. This tells me that teams are more likely to move through the task quicker if given Ind_Team or Team feedback, not individual feedback. 


#### Summary / Reflection

Overall, the distribution is telling me that over time, teams move through the task quicker when given Team or Ind_Team feedback but not individual feedback. This suggests that if I looked at the fourth session and group the data by the feedback conditions, I would see that the time remaining in the Ind_Team and Team condition would be different from the time remaining in the individual condition.

### Q: Is there an *interaction* between the Target and Session Order for the *time remaining*?

I am interested in this question for modeling purposes. I want to know if I need to include the interaction between Target and Session Order. 

#### Team

```{r fig.width= 10}
dependent_response_team <- "timeRemaining_team"
y_label_team <- "Time Remaining"
x_label_team <- "Session Order"
title_response_team <- "Time Remaining (Team) Vs. Target"
value_threshold <- 120
plot_name <- "Bar_TimeRemaining_ByTarget_BySessionOrder_Team.png"
setwd(figure_directory)

plot_data_team <- team_data %>%
  select(Target, SessionOrder, dependent_response_team, Team) %>%
  mutate(rank_order = -rank(team_data[[dependent_response_team]], ties.method = "first")) %>%
  mutate(above_value_threshold = .data[[dependent_response_team]] > value_threshold)

names <- ifelse(plot_data_team[,"above_value_threshold"], as.character( plot_data_team[["Team"]]), "")

ggplot(data = plot_data_team, 
       aes(x = factor(SessionOrder), 
                  y = .data[[dependent_response_team]], 
                  fill = Team, 
                  group = rank_order,
                  label = names )) +
  geom_text(position = position_dodge(1), vjust = 0, aes(y = .data[[dependent_response_team]] + 15), check_overlap = FALSE) +
  geom_bar(stat = "identity", position = "dodge") + 
  facet_grid(. ~ Target) + 
  guides(fill = FALSE) +
  coord_flip()+
  xlim("4", "3", "2") +
  labs(y = y_label_team, x = x_label_team, title = title_response_team, fill = "Teams") +
  ggsave(filename = plot_name)

sumDataCount <- plot_data_team %>%
  group_by(Target, SessionOrder) %>%
  summarise(N = length(.data[[dependent_response_team]]))

sumDataCount %>%
  kable() %>%
  kable_styling()
```

Visually, the first thing that jumps out to me that there seem to be a few outliers that may be influencing the results. 

Let's call a group "fast" if that group has more than `r value_threshold` seconds left at the end of the session. The teams that are labeled had more than `r value_threshold` seconds (`r value_threshold / 60` minutes) remaining at the end of the session. This value is an arbitrary value.

The groups that are labeled in all the conditions (Target levels) are team 26, 45, 13. Some of these teams data may be outliers. 

__Reflection:__ This figure tells me that I may need to consider removing these teams because they may be outliers. However, I do not have a good reason to remove the values. Instead, I considered teams, that seemed to be outliers, fast teams. These are the teams that had more than `r value_threshold` seconds (`r value_threshold / 60` minutes) remaining. If I want to look at the characterisitics of fast groups, I could use teams 13, 26, and 45 as fast moving teams. 


```{r}
dependent_response_team <- "timeRemaining_team"
y_label_team <- "Time Remaining"
x_label_team <- "Target"
title_response_team <- "Time Remaining (Team) Vs. Target"
plot_name <- "InteractionPlot_TimeRemaining_ByTarget_BySessionOrder_Team.png"
setwd(figure_directory)

plot_data_team <- team_data %>%
  select(Target, SessionOrder, dependent_response_team) %>%
  group_by(SessionOrder, Target) %>%
  summarise(dependentAverage = mean(.data[[dependent_response_team]]), 
            Stdv =sd(.data[[dependent_response_team]]), n = length(.data[[dependent_response_team]]), 
            StEr = sd(.data[[dependent_response_team]]) / sqrt(length(.data[[dependent_response_team]])))

ggplot(data = plot_data_team, aes(x = Target, y = dependentAverage, color = SessionOrder, shape = SessionOrder)) +
  geom_point(size = 3) +
  geom_line(aes(group=SessionOrder, color = SessionOrder)) + 
  geom_errorbar(aes(ymin = dependentAverage - StEr, ymax = dependentAverage + StEr), width = 0.2) +
  labs(y = y_label_team, x = x_label_team, title = title_response_team, color = "Session", shape = "Session") +
  ggsave(filename = plot_name)

```

Visually, there appears to be some level of interaction between Target and SessionOrder for the time remaining. The interaction does not seem to be strong. The graph suggests that the average time remaining at the end of the session increases over time. The increase is particularly noticeable in the Ind_Team condition going from session 3 to session 4.  In other words, when given Ind_Team feedback, the amount of time remaining at the end of each session increases faster. 

__Reflection:__ There seems to be some level of interaction but I am getting the sense that it is not a strong interaction. I'll have to run statistical tests to examine the strength of the effect to determine if there is an interaction effect. 

```{r}
dependent_response_team <- "timeRemaining_team"
y_label_team <- "Time Remaining"
x_label_team <- "Target"
title_response_team <- "Time Remaining (Team) Vs. Session Order"
plot_name <- "InteractionPlot_TimeRemaining_BySessionOrder_ByTarget_Team.png"
setwd(figure_directory)

plot_data_team <- team_data %>%
  select(Target, SessionOrder, dependent_response_team) %>%
  group_by(SessionOrder, Target) %>%
  summarise(DependentAverage = mean(.data[[dependent_response_team]]), 
            Stdv =sd(.data[[dependent_response_team]]), n = length(.data[[dependent_response_team]]), 
            StEr = sd(.data[[dependent_response_team]]) / sqrt(length(.data[[dependent_response_team]])))

ggplot(data = plot_data_team, aes(x = SessionOrder , y = DependentAverage, color = Target, shape = Target)) +
  geom_point(size = 3) +
  geom_line(aes(group=Target, color = Target)) + 
  geom_errorbar(aes(ymin = DependentAverage - StEr, ymax = DependentAverage + StEr), width = 0.2) +
  labs(y = y_label_team, x = x_label_team, title = title_response_team, color = "Session", shape = "Session") +
  ggsave(filename = plot_name)
```

There appears to be an interaction between the Target and Session Order variable. This figure tells me that over time the amount of time remaining at the end of each session increases over time. This figure tells me that in the different session orders, the amount of time remaining is roughly the same in after 4 sessions. However, over time, the amount of time remaining at the end of the session changes differently depending on the feedback condition. 
  
The individual session steadily improves over time. 

The Individual_Team session has a small improvement from session 2 to session 3, but there is a substantial increase from session 3 to session 4. This tells me that for some reason there was little change between session 2 and session 3, but something happened that caused a major improvement from session 3 to session 4. 

The team session showed a substantial increase from session 2 to 3 but a small improvement from session 3 to session 4. This suggests that for some reason there was a major improvement between session 2 and session 3, but there was little gain between session 3 and session 4. This could mean that the feedback was no longer useful between 3 and 4 or that the teams reached their highest performance. 

__Reflection:__ I am getting the session that the team condition encouraged teams to move quicker through the session. There was a substantial increase in the amount of time remaining at the end of the session in the Team condition when moving from session 2 to session 3. I am also getting the sense that the Ind_Team feedback condition may be encouraging a team to move quicker through the task as well. However, it looks like the effect of the Ind_Team feedback takes longer to take effect. Even so, the amount of time remaining at the end of the session ended up being about the same after four sessions in the Ind_Team and the Team condition.


#### Individual

```{r}
dependent_response_ind <- "timeRemaining_ind"
y_label_ind <- "Time Remaining"
x_label_ind <- "Target"
title_response_ind <- "Time Remaining (Individual) Vs. Target"
value_threshold <- 120
plot_name <- "Bar_TimeRemaining_ByTarget_BySessionOrder_Ind.png"
setwd(figure_directory)

plot_data_ind <- ind_data %>%
  select(Target, SessionOrder, dependent_response_ind, Player_ID) %>%
  mutate(rank_order = -rank(.data[[dependent_response_ind]])) %>%
  mutate(above_value_threshold = .data[[dependent_response_ind]] > value_threshold)

names <- ifelse(plot_data_ind[,"above_value_threshold"], as.character( plot_data_ind[["Player_ID"]]), "")

ggplot(data = plot_data_ind, 
       aes(x = factor(SessionOrder), 
                  y = .data[[dependent_response_ind]], 
                  fill = Player_ID, group = rank_order,
                  label = names )) +
  geom_bar(stat = "identity", position = "dodge") + 
  facet_grid(. ~ Target) + 
  guides(fill = FALSE) +
  coord_flip()+
  xlim("4", "3", "2") +
  labs(y = y_label_ind, x = x_label_ind, title = title_response_ind, fill = "Player_ID") +
  ggsave(filename = plot_name)

ggplot(data = plot_data_ind, 
       aes(x = factor(SessionOrder), 
           y = .data[[dependent_response_ind]], 
           fill = Player_ID, group = rank_order,
           label = names )) +
  geom_text_repel(stat = "identity", position = position_jitterdodge(), force = 5) +
  geom_point(position = position_jitterdodge()) +
  facet_grid(. ~ Target) + 
  guides(fill = FALSE) +
  coord_flip()+
  xlim("4", "3", "2") +
  labs(y = y_label_ind, x = x_label_ind, title = title_response_ind, fill = "Player ID") +
  ggsave(filename = "Point_TimeRemaining_ByTarget_BySessionOrder_Ind.png", width = 10)

sumDataCount <- plot_data_ind %>%
  group_by(Target, SessionOrder) %>%
  summarise(N = length(.data[[dependent_response_ind]]))

sumDataCount %>%
  kable() %>%
  kable_styling()

sumCountFastParticipants <- plot_data_ind %>%
  filter(above_value_threshold) %>%
  group_by(Target, SessionOrder) %>%
  summarise(N_fast = length(.data[[dependent_response_ind]])) 

sumCountFastParticipants %>%
  kable() %>%
  kable_styling()

```

Visually, it looks like some participants moved quickly through the task (i.e., possible outliers). It seems like there are more "fast" participants in session for in the Team condition. 

__Reflection:__ It looks like pariticipants were moving slower in the Ind condition when compared to the Ind_Team and Team condition. I'm getting the sense that for some reason the individuals were moving fast in the Team condition. Or, participants were getting faster quicker in the Team condition.


```{r}
dependent_response_ind <- "timeRemaining_ind"
y_label_ind <- "Time Remaining"
x_label_ind <- "Target"
title_response_ind <- "Time Remaining (Individual) Vs. Target"
plot_name <- "InteractionPlot_TimeRemaining_ByTarget_BySessionOrder_Ind.png"
setwd(figure_directory)

plot_data_ind <- ind_data %>%
  select(Target, SessionOrder, dependent_response_ind) %>%
  group_by(SessionOrder, Target) %>%
  summarise(Average = mean(.data[[dependent_response_ind]]), 
            Stdv =sd(.data[[dependent_response_ind]]), 
            n = length(.data[[dependent_response_ind]]), 
            StEr = sd(.data[[dependent_response_ind]]) / sqrt(length(.data[[dependent_response_ind]])))

ggplot(data = plot_data_ind, aes(x = Target, y = Average, color = SessionOrder, shape=SessionOrder)) +
  geom_point(size = 3) +
  geom_line(aes(group=SessionOrder, color = SessionOrder)) + 
  geom_errorbar(aes(ymin = Average - StEr, ymax = Average + StEr), width = 0.2) +
  labs(y = y_label_ind, x = x_label_ind, title = title_response_ind, fill = "Players") + 
  ggsave(filename = plot_name)

```

There appears to be an interaction between Target and Session Order. Overall, the amount of time at the end of the session increases over time. It looks like there is a considerable increase in the amount of time remaining in the Team condition from Session 2 to session 3. This substantial increase suggests that giving team feedback resulted in individuals moving faster. It looks like something is happening in the Team and Ind_Team condition. In the Ind condition, there is a steady increase in the amount of time remaining. In the Ind_Team condition, there was little to no change in the time remaining between session 2 and 3 but a big change from session 3 to 4. 

__Reflection:__ I am getting the sense that something is happening with the Ind_Team and the Team condition. For some reason, there are these large increases in the time remaining. 

```{r}
dependent_response_ind <- "timeRemaining_ind"
y_label_ind <- "Time Remaining"
x_label_ind <- "Target"
title_response_ind <- "Time Remaining (Individual) Vs. Target"
plot_name <- "InteractionPlot_TimeRemaining_BySessionOrder_ByTarget_Ind.png"
setwd(figure_directory)

plot_data_ind <- ind_data %>%
  select(Target, SessionOrder, dependent_response_ind) %>%
  group_by(SessionOrder, Target) %>%
  summarise(Average = mean(.data[[dependent_response_ind]]), 
            Stdv =sd(.data[[dependent_response_ind]]), 
            n = length(.data[[dependent_response_ind]]), 
            StEr = sd(.data[[dependent_response_ind]]) / sqrt(length(.data[[dependent_response_ind]])))

ggplot(data = plot_data_ind, aes(x = SessionOrder, y = Average, color = Target, shape=Target)) +
  geom_point(size = 3) +
  geom_line(aes(group=Target, color = Target)) + 
  geom_errorbar(aes(ymin = Average - StEr, ymax = Average + StEr), width = 0.2) +
  labs(y = y_label_ind, x = x_label_ind, title = title_response_ind, fill = "Players") +
  ggsave(filename = plot_name)
```

There seems to be an interaction between the session order and the target variable. 

In the Ind condition, there is a steadying increase in the time remaining at the individual level. 

In the Team condition, there is a large improvement from session 2 to session 3 but little to no improvement from session 3 to session 4. 

In the Ind_Team condition, there is little to no improvement from session 2 to session 3 but a large improvement from session 3 to session 4. 

__Reflection:__ I get the sense that something interesting is going on with the Ind_Team and Team conditions. The figure suggests that both sessions have a positive influence (i.e., increase the amount of time remaining) on how fast participants move through the session. It seems as though the positive effect of the Team condition happens quicker than the impact of the Ind_Team condition.

#### Summary / Reflection

I am getting the sense that there is something interesting happening with the Ind_Team condition and Team condition. For some reason, the positive effect (i.e., the large increase in the time remaining at the end of the session) in the Team condition happens quicker than the positive effect from the Ind_Team condition. More exploration is needed to understand why this apparent effect is happening. Some statistical analysis is necessary to determine the strength of this effect if it exists. Also, statistical tests are needed to determine if there is an interaction between the Target and SessionOrder variable for the individual and team level.


### Q: Is there an interaction between the Target and Session Order for the *time remaining* that is statistically significant?

I am interested in determining if a linear mixed effect model should include the interaction effect between the Target and Session Order variable.

There are a few steps I will take. First, I will fit the multiple models using the data provided. Second, I will compare those models to the null model. Third, I will pick the models that are worth exploring further (e.g., models that are significantly different from the null model). Fourth, I will evaluate the effect size ($R^2$) of the fixed effect and random effect variables using a method by [@Nakagawa2013, @Johnson2014]. Lastly, I will analyze the assumptions for the residuals. 

A note on the effect size ($R^2$). This method generates two types of ($R^2$) values called marginal ($R_{m}^2$) and conditional ($R_{c}^2$) effect size. The $R_{m}^2$ calculates how much of the variance is described by the fixed effect variables (feedback condition and session order) while the $R_{c}^2$ calculates how much of the variance is described by both the fixed and random effect variables.

#### Full Models Used for Team and Individual

The next two sections below describe the full model for team and individual level analysis. Five models were fitted and then compared. One model was the null model, one model was the full model, and the last three are a subset of the full model.

##### Team Full Model

$$ y_{ijt} = \mu + \alpha_{i} + \beta_{j} + \alpha_{i} \beta_{j} + \gamma_{t} +   \epsilon_{ijt}$$ 

Where $y_{ijt}$ is the response variable (e.g., team score), $\mu$ is the baseline (or intercept), $\alpha_{i}$ is the fixed effect for the $i^(th)$ feedback target category, $\beta_{j}$ is the fixed effect for the $j^(th)$ session order, $\gamma_{t}$ is the random effect for the $t^(th)$ team, $ \alpha_{i} \beta_{j}$ is the fixed effect of the interaction between Target and Session Order, and $\epsilon_{ijt}$ is the residual for the model. 

##### Individual Team Full Model

$$ y_{ijtp} = \mu + \alpha_{i} + \beta_{j} + \alpha_{i} \beta_{j} + \gamma_{t} + \theta_{p} + \epsilon_{ijtp}$$ 

Where $y_{ijtp}$ is the response variable (e.g., individual score), $\mu$ is the baseline (or intercept), $\alpha_{i}$ is the fixed effect for the $i^(th)$ feedback target category, $\beta_{j}$ is the fixed effect for the $j^(th)$ session order, $\gamma_{t}$ is the random effect for the $t^(th)$ team, $\theta_{p}$ is the random effect for the $p^(th)$ individual, $ \alpha_{i} \beta_{j}$ is the fixed effect of the interaction between Target and Session Order, and $\epsilon_{ijtp}$ is the residual for the model.


#### Team

```{r message=FALSE}
# Response variable
response_variable <- "timeRemaining_team"

# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         response_variable,
         Target, 
         SessionOrder, 
         Team)

# Generate models with variable response
data_focus_team <- data_modified_team
model.null <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "null", is.team = TRUE, is.robust =FALSE)
model.All <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "All", is.team = TRUE, is.robust = FALSE)
model.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = FALSE)
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = TRUE, is.robust = FALSE)
model.NoInteraction.NoSession <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoSession", is.team = TRUE, is.robust = FALSE)

# There is an option to compare all the models with the anova(). However, the function does not compare all models to model.null
anova(model.null,
      model.All) %>%
  kable(caption = "ANOVA: Null and All") %>%
  kable_styling()

anova(model.null,
      model.NoInteraction) %>%
  kable(caption = "ANOVA: Null and No.Interaction") %>%
  kable_styling()

anova(model.null,
      model.NoInteraction.NoTarget) %>%
  kable(caption = "ANOVA: Null and No.Interaction.No.Target") %>%
  kable_styling()

anova(model.null,
      model.NoInteraction.NoSession) %>%
  kable(caption = "ANOVA: Null and No.Interaction.No.Session") %>%
  kable_styling()

```

The following models were significantly different from the null model: model.All,  model.NoInteraction, model.NoInteraction.NoTarget. 

model.All: Session Order, Target, and the interaction are included in this model.

model.NoInteraction: Session order and target are the fixed effects in this model.

model.NoInteraction.NoTarget: Session order is the only fixed effect in this model.

I want to compare the models that are significanntly different from the null model (i.e., the models mentioned above) and compare them with one another. The goal is to see if the extra terms improve the model. 


```{r}
# Response variable
response_variable <- "timeRemaining_team"

# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         response_variable,
         Target, 
         SessionOrder, 
         Team)

# Generate models with variable response
data_focus_team <- data_modified_team
model.null <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "null", is.team = TRUE, is.robust =FALSE)
model.All <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "All", is.team = TRUE, is.robust = FALSE)
model.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = FALSE)
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = TRUE, is.robust = FALSE)
model.NoInteraction.NoSession <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoSession", is.team = TRUE, is.robust = FALSE)

# Compare all of the models
anova(model.All,
      model.NoInteraction) %>%
  kable(caption = "ANOVA: model.All and model.NoInteraction") %>%
  kable_styling()

anova(model.All,
      model.NoInteraction.NoTarget) %>%
  kable(caption = "ANOVA: model.All and model.NoInteraction.NoTarget") %>%
  kable_styling()

anova(model.NoInteraction,
      model.NoInteraction.NoTarget) %>%
  kable(caption = "ANOVA: model.NoInteraction and model.NoInteraction.NoTarget") %>%
  kable_styling()
```

The results show that the model with the interaction (i.e., Model.All) was not significantly better at describing the data than the other two models (*i.e.,model.NoInteraction.NoTarget and model.NoInteraction*). According to this result, we can remove the interaction effect of the model. It is important to note that some models, compared to model.All, have a small difference in AIC values. However, there is a noticeable difference in BIC values. 

The results also show that the model.NoInteraction is not significantly different from model.NoInteraction.NoTarget. This lack of significance suggests that the model.NoInteraction.NoTarget describes the data best. I notice there is a small difference in the AIC (`r anova(model.NoInteraction, model.NoInteraction.NoTarget)["model.NoInteraction","AIC"] - anova(model.NoInteraction, model.NoInteraction.NoTarget)["model.NoInteraction.NoTarget","AIC"]`) and the BIC (`r anova(model.NoInteraction, model.NoInteraction.NoTarget)["model.NoInteraction","BIC"] - anova(model.NoInteraction, model.NoInteraction.NoTarget)["model.NoInteraction.NoTarget","BIC"]`) criteria values. Due to the small AIC and BIC difference between the two models, I cannot confidently say one model is clearly better than the other model, though the model.NoInteraction.NoTarget is a slightly better model.

I want to calculate the effect size of both models to see if there is a noticeable difference between the effect sizes.


```{r}
# Generate models with variable response
response_variable <- "timeRemaining_team"
data_focus_team <- data_modified_team
model.null <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "null", is.team = TRUE, is.robust =FALSE)
model.All <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "All", is.team = TRUE, is.robust = FALSE)
model.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = FALSE)
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = TRUE, is.robust = FALSE)
model.NoInteraction.NoSession <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoSession", is.team = TRUE, is.robust = FALSE)

# Effect size (R^2)

r.squaredGLMM(model.NoInteraction.NoTarget) %>%
  kable(caption = "Table: Effect size for *No Interaction and No Target*") %>%
   kable_styling(full_width = F)

r.squaredGLMM(model.NoInteraction) %>%
  kable(caption = "Table: Effect size for *No Interaction*") %>%
   kable_styling(full_width = F)
```

There is little difference between the effect sizes for both models. The results show that model.NoInteraction has a slightly better effect size, but the difference is not noticeably different. 


__Reflection:__ The models that are significantly different from the null model are model.All, model.NoInteraction, model.NoInteraction.NoTarget. Comparing these three models showed that the model.All was not significantly better than model.NoInteraction or model.NoInteraction.NoTarget. Comparing model.NoInteraction and model.NoInteraction.NoTarget showed no significant difference between the models (i.e., model.NoInteraction did not explain the data better than model.NoInteraction.NoTarget), suggesting that adding the Target variable did not significantly improve the model. However, I noted that the AIC and BIC values were similar. The effect size for both models was examined to determine if one model had a noticeably higher effect size than the other model. There was no noticeable difference in effect size, but the results indicated that model.NoInteraction had a slightly higher effect size. Model.NoInteraction is the model that I am interested in but I cannot I cannot confidently ignore the model of least interest (i.e., model.NoInteraction.NoTarget). Therefore, I will include model.NoInteraction.NoTarget in further analysis.



#### Individual

```{r, message=FALSE}
# Response Variable

response_variable <- "timeRemaining_ind"

# Data to model
data_modified_ind <- ind_data %>%
  select(response_variable,
         Target, 
         SessionOrder, 
         Team, 
         Player_ID)

data_focus_ind <- data_modified_ind
model.null <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "null", is.team = FALSE, is.robust = FALSE)
model.All <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "All", is.team = FALSE, is.robust = FALSE)
model.NoInteraction <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction", is.team = FALSE, is.robust = FALSE)
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = FALSE, is.robust = FALSE)
model.NoInteraction.NoSession <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction_NoSession", is.team = FALSE, is.robust = FALSE)

# There is an option to compare all the models with the anova(). However, the function does not compare all models to model.null
anova(model.null,
      model.All) %>%
  kable(caption = "ANOVA: model.null and model.All") %>%
  kable_styling()

anova(model.null,
      model.NoInteraction) %>%
  kable(caption = "ANOVA: model.null and model.NoInteraction") %>%
  kable_styling()

anova(model.null,
      model.NoInteraction.NoTarget) %>%
  kable(caption = "ANOVA: model.null and model.NoInteraction.NoTarget") %>%
  kable_styling()

anova(model.null,
      model.NoInteraction.NoSession) %>%
  kable(caption = "ANOVA: model.null and model.NoInteraction.NoSession") %>%
  kable_styling()

```

All of the models were significantly different from the null model (i.e., model.All, model.NoInteraction, model.NoInteraction.NoTarget, and model.NoInteraction.NoSession).

model.All: Session Order, Target, and the interaction are included in this model.

model.NoInteraction: Session order and target are the fixed effects in this model.

model.NoInteraction.NoTarget: Session order is the only fixed effect in this model.

model.NoInteraction.NoSession: Target is the only fixed effect in this model. 

```{r}
# Response Variable

response_variable <- "timeRemaining_ind"

# Data to model
data_modified_ind <- ind_data %>%
  select(response_variable,
         Target, 
         SessionOrder, 
         Team, 
         Player_ID)

data_focus_ind <- data_modified_ind
model.All <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "All", is.team = FALSE, is.robust = FALSE)
model.NoInteraction <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction", is.team = FALSE, is.robust = FALSE)
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = FALSE, is.robust = FALSE)
model.NoInteraction.NoSession <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction_NoSession", is.team = FALSE, is.robust = FALSE)

anova(model.All,
      model.NoInteraction) %>%
  kable(caption = "ANOVA: All and No.Interaction") %>%
  kable_styling()

anova(model.All,
      model.NoInteraction.NoTarget) %>%
  kable(caption = "ANOVA: All and NoInteraction.NoTarget") %>%
  kable_styling()

anova(model.All,
      model.NoInteraction.NoSession) %>%
  kable(caption = "ANOVA: All and NoInteraction.NoSession") %>%
  kable_styling()

anova(model.NoInteraction,
      model.NoInteraction.NoTarget) %>%
  kable(caption = "ANOVA: No.Interaction and NoInteraction.NoTarget") %>%
  kable_styling()

anova(model.NoInteraction,
      model.NoInteraction.NoSession) %>%
  kable(caption = "ANOVA: No.Interaction and NoInteraction.NoSession") %>%
  kable_styling()

anova(model.NoInteraction.NoTarget,
      model.NoInteraction.NoSession) %>%
  kable(caption = "ANOVA: NoInteraction.NoTarget and NoInteraction.NoSession") %>%
  kable_styling()
```

The results show that model.NoInteraction is significantly different from model.NoInteraction.NoTarget and NoInteraction.NoSession. The significant difference means that the model should include the effect of Target and Session Order. The results show that model.All is not significantly from model.NoInteraction. This result tells me that I can confidently remove the interaction effect from the model. I will now calculate the effect size of model.All and model.NoInteraction to examine if there is a noticeable difference between the effect sizes.


```{r}
# Response Variable

response_variable <- "timeRemaining_ind"

# Data to model
data_modified_ind <- ind_data %>%
  select(response_variable,
         Target, 
         SessionOrder, 
         Team, 
         Player_ID)

data_focus_ind <- data_modified_ind
model.All <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "All", is.team = FALSE, is.robust = FALSE)
model.NoInteraction <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction", is.team = FALSE, is.robust = FALSE)

# Effect size (R^2)

r.squaredGLMM(model.NoInteraction) %>%
  kable(caption = "Table: Effect size for *No Interaction*") %>%
   kable_styling(full_width = F)

r.squaredGLMM(model.All) %>%
  kable(caption = "Table: Effect size for *All*") %>%
   kable_styling(full_width = F)
```


__Reflection:__ Ultimately, the data suggests that the model.NoInteraction is the best model to use. It is significantly different from the mode.Null but model.All (i.e., including the interaction effect) does significantly improve the model.


#### Summary / Reflection

At the team level, the models that seem to fit the data best is __model.NoInteraction.NoTarget__ and __model.NoInteraction__. The answer to the original question (is the interaction significant) is that the results suggest the model should not include the interaction effect.

At the individual level, the model that seems to fit the data best is __model.NoInteraction__. The answer to the original question is that the results suggest the model should not include the interaction effect.

My next step is to examine the models and see what I can learn from these models. Are any of the effects in these models significantly different from zero?


### Q: Are any of the effects in the models statistically different from zero?

The purpose of this question is to see what we can learn from these models. Before I examine the model by generating diagnostic plots for the models. The assumptions are as follows [@Galwey2014]:

1. The residuals should have an approximately normal distribution (i.e., a bell curve) when plotted on a histogram.
2. A fitted-value plot should show an almost constant width when viewed from left to right.
3. The points on a normal plot should lie on an almost straight line from the bottom left to the top right.

The models may violate some of the assumptions. According to @Field2017, the best way to examine the influence of assumption violations is to compare a robust model to a standard model. When models violate assumptions, I will compare a robust model to a classic model. If there is a noticeable difference between the two models (i.e., if the estimated coefficient are noticeably different) then I will use a step on the power ladder to transform (@Tukey1977), or re-express, the data to address the assumption violation.

#### Team

We learned that __model.NoInteraction.NoTarget__ and __model.NoInteraction__ best describe the team level data at the team level (see previous analysis). The model __model.NoInteraction__ is the model of interest but I cannot ignore __model.NoInteraction.NoTarget__.

```{r}
# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         timeRemaining_team,
         Target, 
         SessionOrder, 
         Team)

# Plot location
setwd(figure_directory)

# Generate models with variable response
response_variable <- "timeRemaining_team"
data_focus_team <- data_modified_team
model.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = FALSE)

# Histogram of Residuals - model.NoInteraction
ggplot(model.NoInteraction, aes(x = .resid)) +
  geom_histogram(bins = 30) + 
  labs(title = paste("Histogram of Residuals for model.NoInteraction"), x = "", y = "") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggsave(filename = "Histogram_Residuals_TimeRemaining_NoInteraction_Team.png")

# Fitted values - model.NoInteraction
ggplot(model.NoInteraction, aes(x = .fitted, y = .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0)+
  labs(title = paste("Fitted-Value Plot for model.NoInteraction"), x = "Fitted Values", y = "Residuals") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggsave(filename = "FitteValue_Residuals_TimeRemaining_NoInteraction_Team.png")

# QQ plots - model.NoInteraction
ggplot(model.NoInteraction, aes(sample = .resid)) +
  stat_qq() +
  stat_qq_line()+
  labs(title = "Normal Q-Q Plot for NoInteraction", x = "Theoretical", y = "Sample") +
   theme(plot.title = element_text(hjust = 0.5)) +
  ggsave(filename = "QQ_Residuals_TimeRemaining_NoInteraction_Team.png")
```

model.NoInteraction: Visually, it looks like the model violates assumption 2 and 3 in the fitted-value plot and the QQ plot, respectively. I will need to compare this model to a robust model to examine the influence of the apparent violation. If there is a noticeable difference, I will need to transform the data.


```{r}
# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         timeRemaining_team,
         Target, 
         SessionOrder, 
         Team)

# Plot location
setwd(figure_directory)

# Generate models with variable response
response_variable <- "timeRemaining_team"
data_focus_team <- data_modified_team
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = TRUE, is.robust = FALSE)

# Histogram of Residuals - model.NoInteraction.NoTarget
ggplot(model.NoInteraction.NoTarget, aes(x = .resid)) +
  geom_histogram(bins = 30) + 
  labs(title = paste("Histogram of Residuals for model.NoInteraction.NoTarget"), x = "", y = "") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggsave(filename = "Histogram_Residuals_TimeRemaining_NoInteraction.NoTarget_Team.png")


# Fitted values - model.NoInteraction.NoTarget
ggplot(model.NoInteraction.NoTarget, aes(x = .fitted, y = .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0)+
  labs(title = paste("Fitted-Value Plot for model.NoInteraction.NoTarget"), x = "Fitted Values", y = "Residuals") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggsave(filename = "FittedValue_Residuals_TimeRemaining_NoInteraction.NoTarget_Team.png")

# QQ plots - model.NoInteraction.NoTarget
ggplot(model.NoInteraction.NoTarget, aes(sample = .resid)) +
  stat_qq() +
  stat_qq_line()+
  labs(title = "Normal Q-Q Plot for NoInteraction.NoTarget", x = "Theoretical", y = "Sample") +
   theme(plot.title = element_text(hjust = 0.5)) +
  ggsave(filename = "QQ_Residuals_TimeRemaining_NoInteraction.NoTarget_Team.png")
```

model.NoInteraction.NoTarget: Visually, it looks like the model violates assumption 2 and 3 in the fitted-value plot and the QQ plot, respectively. I will need to compare this model to a robust model to examine the influence of the apparent violation. If there is a noticeable difference, I will need to transform the data.

__Reflection:__ It looks both models violated assumptions 2 and 3. In other words,  there may be a constant error violation (i.e., the fitted value plot is not consistent from left to right) and the QQ plot shows an assumption violation. I need to determine if this violation influences the model overall.


##### Q: Does the violation influence the linear model estimation?

I will use the robust estimation for linear mixed models found in the robustlmm package by @Koller2016. I will then compare the robust model to the non-robust model. If the estimated values are noticeably different, then data will need to be transformed. 


```{r}
# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         timeRemaining_team,
         Target, 
         SessionOrder, 
         Team)

response_variable <- "timeRemaining_team"
data_focus_team <- data_modified_team

# Generate robust and non-robust models - NoInteraction
model.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = FALSE)
rmodel.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = TRUE)

comparision_results <- compare(model.NoInteraction, rmodel.NoInteraction, show.rho.functions = FALSE)

comparision_results %>%
  kable(caption = "model.NoInteraction") %>%
  kable_styling()

```

The robust model.NoInteraction model is noticeably different from the non-robust model.NoInteraction. This suggests that the data should be transformed to satisfy the residual assumptions.

```{r}
# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         timeRemaining_team,
         Target, 
         SessionOrder, 
         Team)

response_variable <- "timeRemaining_team"
data_focus_team <- data_modified_team

# Generate robust and non-robust models - NoInteraction.NoTarget
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = TRUE, is.robust = FALSE)
rmodel.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = TRUE, is.robust = TRUE)

comparision_results <- compare(model.NoInteraction.NoTarget, rmodel.NoInteraction.NoTarget, show.rho.functions = FALSE)

comparision_results %>%
  kable(caption = "model.NoInteraction.NoTarget") %>%
  kable_styling()
```

The robust model.NoInteraction.NoTarget model is noticeably different from the non-robust model.NoInteraction.NoTarget This suggests that the data should be transformed to satisfy the residual assumptions.

__Reflection:__ Comparing the normal estimation to the robust models indicates that the data should be transformed because the estimates are noticeably different.

##### Q: Does data transformation satisfy the assumptions of the residuals?

I will use the square root transformation method from Tukey's ladder [@Tukey1977]. 

```{r}
# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         timeRemaining_team,
         Target, 
         SessionOrder, 
         Team) %>%
  mutate(sqrt.value = sqrt(.data[["timeRemaining_team"]]))

response_variable <- "sqrt.value"
data_focus_team <- data_modified_team

#Plot location
setwd(figure_directory)

# Generate non-robust models - NoInteraction
model.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = FALSE)

# Histogram of Residuals - model.NoInteraction
ggplot(model.NoInteraction, aes(x = .resid)) +
  geom_histogram(bins = 30) + 
  labs(title = paste("Histogram of Residuals for model.NoInteraction"), x = "", y = "") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggsave("Histogram_Residuals_TimeRemaining_sqrt_NoInteraction_Team.png")

# Fitted values - model.NoInteraction
ggplot(model.NoInteraction, aes(x = .fitted, y = .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0)+
  labs(title = paste("Fitted-Value Plot for model.NoInteraction"), x = "Fitted Values", y = "Residuals") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggsave("FitteValue_Residuals_TimeRemaining_sqrt_NoInteraction_Team.png")

# QQ plots - model.NoInteraction
ggplot(model.NoInteraction, aes(sample = .resid)) +
  stat_qq() +
  stat_qq_line()+
  labs(title = "Normal Q-Q Plot for model.NoInteraction", x = "Theoretical", y = "Sample") +
   theme(plot.title = element_text(hjust = 0.5)) + 
  ggsave("QQ_Residuals_TimeRemaining_sqrt_NoInteraction_Team.png")
```

The diagnostic plots suggest the transformation has resolved the assumption violations for the model.NoInteraction.

```{r}
# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         timeRemaining_team,
         Target, 
         SessionOrder, 
         Team) %>%
  mutate(sqrt.value = sqrt(.data[["timeRemaining_team"]]))

response_variable <- "sqrt.value"
data_focus_team <- data_modified_team

model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = TRUE, is.robust = FALSE)

# Plot location 
setwd(figure_directory)

# Histogram of Residuals - model.NoInteraction.NoTarget
ggplot(model.NoInteraction.NoTarget, aes(x = .resid)) +
  geom_histogram(bins = 30) + 
  labs(title = paste("Histogram of Residuals for model.NoInteraction.NoTarget"), x = "", y = "") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggsave("Histogram_Residuals_TimeRemaining_sqrt_NoInteraction.NoTarget_Team.png")

# Fitted values - model.NoInteraction.NoTarget
ggplot(model.NoInteraction.NoTarget, aes(x = .fitted, y = .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0)+
  labs(title = paste("Fitted-Value Plot for model.NoInteraction.NoTarget"), x = "Fitted Values", y = "Residuals") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggsave("FittedValue_Residuals_TimeRemaining_sqrt_NoInteraction.NoTarget_Team.png")

# QQ plots - model.NoInteraction.NoTarget
ggplot(model.NoInteraction.NoTarget, aes(sample = .resid)) +
  stat_qq() +
  stat_qq_line()+
  labs(title = "Normal Q-Q Plot for NoInteraction.NoTarget", x = "Theoretical", y = "Sample") +
   theme(plot.title = element_text(hjust = 0.5)) +
  ggsave("QQ_Residuals_TimeRemaining_sqrt_NoInteraction.NoTarget_Team.png")
```

The diagnostic plots suggest the transformation has resolved the assumption violations for the model.NoInteraction.NoTarget.

__Reflection:__ The plots suggest that the data transformation resolved the residual assumption violations for both models. I will use a step on Tukey's ladder to transform the data.

##### Q: Are there any effects that are significant?

I want to determine if Target has a significant effect. I will examine the model itself, and then I will calculate the estimated marginal means to compare all three levels.

```{r}
# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         timeRemaining_team,
         Target, 
         SessionOrder, 
         Team) %>%
  mutate(sqrt.value = sqrt(.data[["timeRemaining_team"]]))

response_variable <- "sqrt.value"
data_focus_team <- data_modified_team

# Generate robust and non-robust models - NoInteraction
model.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = FALSE)

summary(model.NoInteraction)
```

The estimations are in reference to the Ind level for the Target variable and session 2 for the session order variable. The results suggest that session 3 and 4 have a significant effect. I expected this effect. The results also indicate that the impact of the Team and Ind_Team levels do not have an effect that is greater than zero, in reference to the Ind level.

```{r}
# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         timeRemaining_team,
         Target, 
         SessionOrder, 
         Team) %>%
  mutate(sqrt.value = sqrt(.data[["timeRemaining_team"]]))

response_variable <- "sqrt.value"
data_focus_team <- data_modified_team

# Generate robust and non-robust models - NoInteraction
model.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = FALSE)

emmeans(model.NoInteraction, list(pairwise ~ Target), adjust = "tukey")
```
The estimated marginal means show no significant difference among the Target variable. It does look like the different between Ind_Team and Team is approaching significance (p = .168). This approaching significance suggests that more data is needed.

```{r}
# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         timeRemaining_team,
         Target, 
         SessionOrder, 
         Team) %>%
  mutate(sqrt.value = sqrt(.data[["timeRemaining_team"]]))

response_variable <- "sqrt.value"
data_focus_team <- data_modified_team

# Generate robust and non-robust models - NoInteraction
model.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = FALSE)

emmeans(model.NoInteraction, list(pairwise ~ SessionOrder), adjust = "tukey")
```

The estimated marginal means show a significant difference between all of the sessions. This significance suggests that the amount of time remaining at the end of the session increased over time. In other words, the teams were getting faster over time.

__Reflection:__ The *model.NoInteraction:* model shows no significant effect from the Target levels, but it does show a significant effect from the session order. Specifically, all of the session orders were significantly different from one another. The significance among the session orders means that the amount of time remaining at the end of each session increased over time. 

```{r}
# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         timeRemaining_team,
         Target, 
         SessionOrder, 
         Team) %>%
  mutate(sqrt.value = sqrt(.data[["timeRemaining_team"]]))

response_variable <- "sqrt.value"
data_focus_team <- data_modified_team

# Generate robust and non-robust models - NoInteraction_NoTarget
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = TRUE, is.robust = FALSE)

summary(model.NoInteraction.NoTarget)
```

The results show that that session 3 and 4 are significantly different from session 2.

```{r}
# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         timeRemaining_team,
         Target, 
         SessionOrder, 
         Team) %>%
  mutate(sqrt.value = sqrt(.data[["timeRemaining_team"]]))

response_variable <- "sqrt.value"
data_focus_team <- data_modified_team

# Generate robust and non-robust models - NoInteraction_NoTarget
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = TRUE, is.robust = FALSE)

emmeans(model.NoInteraction.NoTarget, list(pairwise ~ SessionOrder), adjust = "tukey")
```

The results show a significant difference between each session. 

__Reflection:__ As expected, the results show a significant difference among the session order. In other words, the amount of time remaining at the end of each session increased over time. 

##### Summary / Reflection

I had to transform the data to resolve assumption issues. I used one of the steps in the Tukey ladder @Tukey1977. I took the square root of all the values in the time remaining value for the teams.

Overall, the results show no significant effect from the Target variable, but it does show that the session order has a significant impact. This impact means that the amount of time remaining at the end of each session increased over time. I expected the effects of Session Order. There are no statistically significant results that I can discuss about Target.

#### Individual

We learned that __model.NoInteraction__ best describe the individual level data(see previous analysis).

```{r}
# Data to model
data_modified_ind <- ind_data %>%
  select(timeRemaining_ind,
         Target, 
         SessionOrder, 
         Team, 
         Player_ID)

# Plot location
setwd(figure_directory)

# Generate models with variable response
response_variable <- "timeRemaining_ind"
data_focus_ind <- data_modified_ind
model.NoInteraction <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction", is.team = FALSE, is.robust = FALSE)

# Histogram of Residuals - model.NoInteraction
ggplot(model.NoInteraction, aes(x = .resid)) +
  geom_histogram(bins = 30) + 
  labs(title = paste("Histogram of Residuals for model.NoInteraction"), x = "", y = "") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggsave("Histogram_Residuals_TimeRemaining_NoInteraction_Ind.png")

# Fitted values - model.NoInteraction
ggplot(model.NoInteraction, aes(x = .fitted, y = .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0)+
  labs(title = paste("Fitted-Value Plot for model.NoInteraction"), x = "Fitted Values", y = "Residuals") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggsave("FittedValue_Residuals_TimeRemaining_NoInteraction_Ind.png")

# QQ plots - model.NoInteraction
ggplot(model.NoInteraction, aes(sample = .resid)) +
  stat_qq() +
  stat_qq_line()+
  labs(title = "Normal Q-Q Plot for NoInteraction", x = "Theoretical", y = "Sample") +
   theme(plot.title = element_text(hjust = 0.5)) +
  ggsave("QQ_Residuals_TimeRemaining_NoInteraction_Ind.png")
```

model.NoInteraction: Visually, it looks like the model violates assumption 2 and 3 in the fitted-value plot and the QQ plot, respectively. I will need to compare this model to a robust model to examine the influence of the apparent violation. If there is a noticeable difference, I will need to transform the data.

__Reflection:__ Overall, it looks like I need to compare the model to the robust models to see if the violation has any influence on the model estimation.

##### Q: Does the violation influence the model estimation?

I will use the robust estimation for linear mixed models found in the robustlmm package by @Koller2016. I will then compare the robust model to the non-robust model. If the estimated values are noticeably different, then data will need to be transformed. 

```{r}
# Data to model
data_modified_ind <- ind_data %>%
  select(timeRemaining_ind,
         Target, 
         SessionOrder, 
         Team, 
         Player_ID)

# Generate models with variable response
response_variable <- "timeRemaining_ind"
data_focus_ind <- data_modified_ind
model.NoInteraction <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction", is.team = FALSE, is.robust = FALSE)
rmodel.NoInteraction <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction", is.team = FALSE, is.robust = TRUE)

comparision_results <- compare(model.NoInteraction, rmodel.NoInteraction, show.rho.functions = FALSE)

comparision_results %>%
  kable(caption = "model.NoInteraction") %>%
  kable_styling()
```

There is a noticeable difference between the non-robust model and the robust model. This noticeable difference indicates that the data should be transformed to attempt to satisfy the assumptions.

__Reflection:__ I need to transform the data because there is a very noticeable difference between the robust model and the non-robust model.

##### Q: Does data transformation satisfy the assumptions of the residuals?

I will use the square root transformation method from Tukey's ladder [@Tukey1977]. 

```{r}
# Data to model
data_modified_ind <- ind_data %>%
  select(timeRemaining_ind,
         Target, 
         SessionOrder, 
         Team, 
         Player_ID) %>%
  mutate(sqrt.value = sqrt(ifelse(.data[["timeRemaining_ind"]] < 0, 0, .data[["timeRemaining_ind"]]))) # Some of the time remaining vaules are negative when the should be zero. I want to make sure those values are zero

# Plot location
setwd(figure_directory)

response_variable <- "sqrt.value"
data_focus_ind <- data_modified_ind

# Generate models with variable response
model.NoInteraction <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction", is.team = FALSE, is.robust = FALSE)

# Histogram of Residuals - model.NoInteraction
ggplot(model.NoInteraction, aes(x = .resid)) +
  geom_histogram(bins = 30) + 
  labs(title = paste("Histogram of Residuals for model.NoInteraction"), x = "", y = "") +
  theme(plot.title = element_text(hjust = 0.5)) + 
  ggsave("Histogram_Residuals_TimeRemaining_sqrt_NoInteraction_Ind.png")

# Fitted values - model.NoInteraction
ggplot(model.NoInteraction, aes(x = .fitted, y = .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0)+
  labs(title = paste("Fitted-Value Plot for model.NoInteraction"), x = "Fitted Values", y = "Residuals") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggsave("FittedValue_Residuals_TimeRemaining_sqrt_NoInteraction_Ind.png")

# QQ plots - model.NoInteraction
ggplot(model.NoInteraction, aes(sample = .resid)) +
  stat_qq() +
  stat_qq_line()+
  labs(title = "Normal Q-Q Plot for NoInteraction", x = "Theoretical", y = "Sample") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggsave("QQ_Residuals_TimeRemaining_sqrt_NoInteraction_Ind.png")
```

There still appears to be a normality violation in the QQ plot. So, I need to compare the robust and non-robust models to make sure I do not need to worry about this violation.

```{r}
# Data to model
data_modified_ind <- ind_data %>%
  select(timeRemaining_ind,
         Target, 
         SessionOrder, 
         Team, 
         Player_ID) %>%
  mutate(sqrt.value = sqrt(ifelse(.data[["timeRemaining_ind"]] < 0, 0, .data[["timeRemaining_ind"]]))) # Some fo the time remaining vaules are negative when the should be zero. I want to make sure those values are zero

response_variable <- "sqrt.value"
data_focus_ind <- data_modified_ind

# Generate models with variable response
model.NoInteraction <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction", is.team = FALSE, is.robust = FALSE)
rmodel.NoInteraction <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction", is.team = FALSE, is.robust = TRUE)

compare(model.NoInteraction, rmodel.NoInteraction, show.rho.functions = FALSE) %>%
  kable() %>%
  kable_styling()
```

Comparing the robust and non-robust model does not reveal any concerning difference in estimated value.

__Reflection:__ After the transformation, there still appeared to be a violation in the QQ plot but comparing the robust model to the non-robust model showed no concerning differences.

##### Q: Are there any effects that are significant?

```{r}
# Data to model
data_modified_ind <- ind_data %>%
  select(timeRemaining_ind,
         Target, 
         SessionOrder, 
         Team, 
         Player_ID) %>%
  mutate(sqrt.value = sqrt(ifelse(.data[["timeRemaining_ind"]] < 0, 0, .data[["timeRemaining_ind"]]))) # Some fo the time remaining vaules are negative when the should be zero. I want to make sure those values are zero

response_variable <- "sqrt.value"
data_focus_ind <- data_modified_ind

# Generate models with variable response
model.NoInteraction <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction", is.team = FALSE, is.robust = FALSE)

summary(model.NoInteraction)
```

The results showed a significant effect from session 3 and 4 when compared to session 2. This significant effect is expected. The results showed no significant impact from Target levels when compared to the Ind level. However, it is important to note that the effect from the Team Target level is approaching significance (p = .0779). This suggests that more data is needed to reach significance. 

```{r}
# Data to model
data_modified_ind <- ind_data %>%
  select(timeRemaining_ind,
         Target, 
         SessionOrder, 
         Team, 
         Player_ID) %>%
  mutate(sqrt.value = sqrt(ifelse(.data[["timeRemaining_ind"]] < 0, 0, .data[["timeRemaining_ind"]]))) # Some fo the time remaining vaules are negative when the should be zero. I want to make sure those values are zero

response_variable <- "sqrt.value"
data_focus_ind <- data_modified_ind

# Generate models with variable response
model.NoInteraction <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction", is.team = FALSE, is.robust = FALSE)

emmeans(model.NoInteraction, list(pairwise ~ Target), adjust = "tukey")
```
The results show a *significant difference* between the Ind_Team and Team level of feedback. The difference indicates that the amount of time remaining in the Team feedback condition results in higher time remaining (transformed using 1/2 power). 

```{r}
# Data to model
data_modified_ind <- ind_data %>%
  select(timeRemaining_ind,
         Target, 
         SessionOrder, 
         Team, 
         Player_ID) %>%
  mutate(sqrt.value = sqrt(ifelse(.data[["timeRemaining_ind"]] < 0, 0, .data[["timeRemaining_ind"]]))) # Some fo the time remaining vaules are negative when the should be zero. I want to make sure those values are zero

response_variable <- "sqrt.value"
data_focus_ind <- data_modified_ind

# Generate models with variable response
model.NoInteraction <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction", is.team = FALSE, is.robust = FALSE)

emmeans(model.NoInteraction, list(pairwise ~ SessionOrder), adjust = "tukey")
```

There is a significant difference between each of the sessions. This result means that the time remaining at the end of the session was increasing over time. This increase in time remaining suggests that the teams were moving through the session faster the more times they completed the task (Note: This is when the data is transformed using the 1/2 power).

##### Summary / Reflection

There was a __significant difference__ between the Ind_Team and Team level feedback condition in __model.NoInteraction__. The significant difference indicates that the amount of time remaining at the end of the session in the Team feedback condition is higher than the amount of time remaining in the Ind_Team condition. 

According to the data, giving groups feedback that contains team performance metrics will encourage them to move through the task faster. The amount of time remaining at the end of the session does not tell me if the groups in the Team condition are accurate (i.e., able to collected correct items quicker) as they are completing the task. A plausible conclusion could be that the groups in the Team condition are merely moving through the session quicker than those in the Ind_Team condition. 

My next step is to explore how well the groups collected items. I want to determine if the groups needed less time to collect correct items in the different conditions (particularly the team condition).

### Q: Is there exciting variation among the *correct items collected* metric that will help me understand the influence of the feedback condition?

#### Team

```{r hist_CountVSCorrectItemsCollected}
dependent_response_team <- "CI_team"
y_label_team <- "Count"
x_label_team <- "Correct items collected"
title_response_team <- "Distribution of Correct Items collected (Team)"
plot_name <- "Histogram_CI_Team.png"
setwd(figure_directory)

plot_data_team <- team_data %>%
  select(dependent_response_team, Target)

ggplot(data = plot_data_team) + 
  geom_histogram(aes_string(x = dependent_response_team), bins = 30) +
  labs(title = title_response_team, x = x_label_team, y = y_label_team) +
  ggsave(filename = plot_name)
```

Overall, there is a skew to the right. Specifically, teams collected at least 13 of 18 items they needed to collect.

```{r }
dependent_response_team <- "CI_team"
y_label_team <- "Count"
x_label_team <- "Target"
title_response_team <- "Distribution of Correct Items collected (Team)"
plot_name <- "Histogram_CI_ByTarget_Team.png"
setwd(figure_directory)

plot_data_team <- team_data %>%
  select(dependent_response_team, Target)

ggplot(data = plot_data_team) + 
  geom_histogram(aes_string(x = dependent_response_team), bins = 30) +
  facet_grid(. ~ Target) +
  labs(title = title_response_team, x = x_label_team, y = y_label_team) +
  ggsave(filename = plot_name)

# Information about the N values in each condition
tableData <- team_data %>%
  group_by(Target) %>%
  summarise(N = length(.data[[dependent_response_team]]))

tableData %>%
  kable() %>%
  kable_styling()
```

Overall, it looks like the data shows a similar distribution when grouped by Target. The data skews to the right. It is interesting to note that the team condition has a slightly larger skew to the right.

__Reflection:__ Overall, the main distribution of the data is similar to the distribution of the data when broken to the different target groups. The interesting note is that the Team condition seems to have a greater skew to the right. This indicates that groups in the Team condition were able to collect more of the correct team items.

#### Individual

```{r}
dependent_response_ind <- "CI_ind"
y_label_ind <- "Count"
x_label_ind <- "Correct items collected"
title_response_ind <- "Distribution of Correct Items collected (Individual)"
plot_name <- "Histogram_CI_Ind.png"
setwd(figure_directory)

plot_data_ind <- ind_data %>%
  select(dependent_response_ind, Target)

ggplot(data = plot_data_ind) + 
  geom_histogram(aes_string(x = dependent_response_ind), bins = 40) +
  labs(title = title_response_ind, x = x_label_ind, y = y_label_ind) +
  ggsave(filename = plot_name)
```

Overall, the data is skewed to the right. Specifically, most participants were able to collect at least 3 or 4 items. 

```{r}
dependent_response_ind <- "CI_ind"
y_label_ind <- "Count"
x_label_ind <- "Correct items collected"
title_response_ind <- "Distribution of Correct Items collected (Individual)"
plot_name <- "Histogram_CI_ByTarget_Ind.png"
setwd(figure_directory)

plot_data_ind <- ind_data %>%
  select(dependent_response_ind, Target)

ggplot(data = plot_data_ind) + 
  geom_histogram(aes_string(x = dependent_response_ind), bins = 30) +
  facet_grid(. ~ Target) +
  labs(title = title_response_ind, x = x_label_ind, y = y_label_ind) +
  ggsave(filename = plot_name)

# Information about the N values in each condition
tableData <- ind_data %>%
  group_by(Target) %>%
  summarise(N = length(.data[[dependent_response_ind]]))

tableData %>%
  kable() %>%
  kable_styling()
```

Overall, it seems as though the distrabution of the data grouped by the target level is similar to the overall distrabution. It is interesting to note that the Ind and Team condition seem to have similar distrabutions. Specifically, most participants were able to collected at least 3 or 4 individual items. In the Ind_Team condition, it seems as though about 30 or 40 participants were were only abel to collect 2 to 4 individual items, while the rest were able to collect about 5 or 6 items. 

__Reflection:__ Overall, it looks like the data distrabution when grouped by Target is similar to the overall distrabution of the data. An interesting note is that the distrabution of the data in the Team and Ind condition are similar. In the Ind_Team condition it seems as though participants were able to collect either 2-4 items or 5 - 6 items. 

#### Summary / Reflection

Overall, there was skewness in the data at the individual and team level. 

At the team level, there seems to be a slightly bigger skew to the right in the Team condition when compared to the Ind_Team and Ind condition. This could be an indiciation that groups in the Team condition were able to collect more team items than in the other conditions. 

At the individual level, it seems as though the Ind and Team condition had similar distrabutions, while the Ind_Team condition seemed to have less skewness than when compared to the Ind and Team condition. This could be an indication that the individuals were able to collect more correct individual items in the Team or Ind condition, but not as much in the Ind_Team condition. 

No obvious pattern jumps out at me. However, its seems as though there may be something interesting happening in the Team condition. What does the distrabution look like if we include session order?

### Q: Is there exciting variation among the *correct items collected* that explains how the feedback conditions influence the *correct items collected* when session order is taken into consideration?

#### Team 

```{r}
dependent_response_team <- "CI_team"
y_label_team <- "Count"
x_label_team <- "Correct items collected"
title_response_team <- "Distribution of Correct Items collected (Team)"
plot_name <- "Histogram_CI_ByTarget_BySessionOrder_Team.png"
setwd(figure_directory)

plot_data_team <- team_data %>%
  select(dependent_response_team, Target, SessionOrder)

ggplot(data = plot_data_team) + 
  geom_histogram(aes_string(x = dependent_response_team), bins = 30) +
  labs(title = title_response_team, x = x_label_team, y = y_label_team) +
  facet_grid( SessionOrder ~ Target) + 
  ggsave(filename = plot_name)

# Information about the N values in each condition
tableData <- team_data %>%
  select(dependent_response_team, Target, SessionOrder) %>%
  group_by(Target, SessionOrder) %>%
  summarise(N = length(.data[[dependent_response_team]]))

tableData %>%
  kable() %>%
  kable_styling()
```

Overall, it looks like the data moves to the right over time. In other words, the participants were able to collect more and more items. Something that may be worth noting is that the distribution of the data in the Ind condition seems to stay constant, as opposed to moving to the right. 

_Refelction:_ The data seems to have a similar distribution as the main overall data distribution. An interesting note is that the Ind condition appears to have the same distribution from session to session. This could indicate that the intervention had no effect on how many items the teams collected or that it reduced the number of items collected by the teams and that negated any performance gained by experience.


#### Individual 

```{r}
dependent_response_ind <- "CI_ind"
y_label_ind <- "Count"
x_label_ind <- "Correct items collected"
title_response_ind <- "Distribution of Correct Items collected (Individual)"
plot_name <- "Histogram_CI_ByTarget_BySessionOrder_Ind.png"
setwd(figure_directory)

plot_data_ind <- ind_data %>%
  select(dependent_response_ind, Target, SessionOrder)

ggplot(data = plot_data_ind) + 
  geom_histogram(aes_string(x = dependent_response_ind), bins = 30) +
  facet_grid(SessionOrder ~ Target) +
  labs(title = title_response_ind, x = x_label_ind, y = y_label_ind) +
  ggsave(filename = plot_name)

# Information about the N values in each condition
tableData <- ind_data %>%
  select(dependent_response_ind, Target, SessionOrder) %>%
  group_by(Target, SessionOrder) %>%
  summarise(N = length(.data[[dependent_response_ind]]))

tableData %>%
  kable() %>%
  kable_styling()
```

Overall, it looks like the data distribution skews to the right over time. 

In the Ind condition, there seems to be little change in the distribution of the data. 

In the Ind_Team condition, session 2 and 3 seem to have similar distributions. Session 4 distribution appears to move more to the right. 

In the Team condition, distribution in session 2 and 3 are similar, but session 4 has more of skew to the right. 

In session 4, it seems as though the Ind condition and Ind_Team condition have a similar distribution and the distribution in the Team condition. 

__Reflection:__ Overall, there appears to be no obvious pattern. It is worth noting that the distribution in the Ind conditions seems to go unchanged from session to session. This could mean that the intervention had no effect or had a negative impact on participants collecting correct individuals items. In the Ind_Team and Team condition, it looks like a major improvement (i.e., a more obvious skew to the right) happens when moving from session 3 to session 4. This indicates that something happens between sessions 3 and 4 to increase performance. It seems as though the skew in the Team condition in session 4 is greater than the skew in the Ind and Ind_Team conditions.

#### Summary / Reflection

Overall, there were no obvious patterns. 

At the individual and team level, the Ind condition seemed to have either no influence or a negative influence on the correct items collected. Also, the Team condition seemed to have the biggest skew (i.e., most data seemed to be to the right) at both levels. This would indicate that the Team condition had a positive influence on the correct items collected over time. 

### Q: Is there an *interaction* between the Target and Session Order for the **correct items collected**?

I am interested in this question for modeling purposes. I want to know if I need to include the interaction between Target and Session Order. 

#### Team
```{r}
dependent_response_team <- "CI_team"
y_label_team <- "Correct items collected"
x_label_team <- "Target"
title_response_team <- "Correct items collected (Team) Vs. Target"
plot_name <- "InteractionPlot_CI_ByTarget_BySessionOrder_Team.png"
setwd(figure_directory)

plot_data_team <- team_data %>%
  select(Target, SessionOrder, dependent_response_team) %>%
  group_by(SessionOrder, Target) %>%
  summarise(dependentAverage = mean(.data[[dependent_response_team]]), 
            Stdv =sd(.data[[dependent_response_team]]), n = length(.data[[dependent_response_team]]), 
            StEr = sd(.data[[dependent_response_team]]) / sqrt(length(.data[[dependent_response_team]])))

ggplot(data = plot_data_team, aes(x = Target, y = dependentAverage, color = SessionOrder, shape = SessionOrder)) +
  geom_point(size = 3) +
  geom_line(aes(group=SessionOrder, color = SessionOrder)) + 
  geom_errorbar(aes(ymin = dependentAverage - StEr, ymax = dependentAverage + StEr), width = 0.2) +
  labs(y = y_label_team, x = x_label_team, title = title_response_team, color = "Session", shape = "Session") +
  ggsave(filename = plot_name)

plot_data_team %>%
  kable() %>%
  kable_styling()
```

Overall, there seems to be something interesting happening among the conditions. Specifically, there appears to be a slow improvement over time in the Ind condition. There appears to be a steady improvement over time in the Ind_Team condition. There appears to be a quick improvement over time in the Team condition when compared to the Ind_Team and Ind condition.

Also, it appears that the highest average for session 2 occurs in the Team condition. This suggests that teams collected more correct items in session 2 when put in the Team condition.

__Reflection:__ There seems to be a trend in the conditions. In each condition, the groups improve their ability to collect correct team items over time. However, the rate in which they improve seems to be different depending on the condition. If this trend is accurate, then groups in the Ind condition improved slowly over time, groups in the Ind_Team condition improve steadily over time, and groups in the Team condition seem to improve quickly over time. 

Overall, there does not seem to be any interaction occurring.

```{r}
dependent_response_team <- "CI_team"
y_label_team <- "Correct items collected"
x_label_team <- "Target"
title_response_team <- "Correct items collected (Team) Vs. Session Order"
plot_name <- "InteractionPlot_CI_BySessionOrder_ByTarget_Team.png"
setwd(figure_directory)

plot_data_team <- team_data %>%
  select(Target, SessionOrder, dependent_response_team) %>%
  group_by(SessionOrder, Target) %>%
  summarise(DependentAverage = mean(.data[[dependent_response_team]]), 
            Stdv =sd(.data[[dependent_response_team]]), n = length(.data[[dependent_response_team]]), 
            StEr = sd(.data[[dependent_response_team]]) / sqrt(length(.data[[dependent_response_team]])))

ggplot(data = plot_data_team, aes(x = SessionOrder , y = DependentAverage, color = Target, shape = Target)) +
  geom_point(size = 3) +
  geom_line(aes(group=Target, color = Target)) + 
  geom_errorbar(aes(ymin = DependentAverage - StEr, ymax = DependentAverage + StEr), width = 0.2) +
  labs(y = y_label_team, x = x_label_team, title = title_response_team, color = "Session", shape = "Session") +
  ggsave(filename = plot_name)
```
Overall, there seems to be an interaction between Session Order and Target. 

The Ind condition seems to improve from session 2 to 3 but stays the same from session 3 to 4. 

The Ind_Team condition steadily improves over time. 

The Team condition improves a little from session 2 to 3 but has a bigger improvement from session 3 to 4. 

__Reflection:__ Overall, there seems to be a pattern to how well the amount of correct items collected increases over time. Generally, the Ind condition appears to improve the least, the Ind_Team condition improves steadily over time, and the Team condition improves the quickest. Overall, it looks like there may be some interaction between the two variables. 

#### Individual

```{r}
dependent_response_ind <- "CI_ind"
y_label_ind <- "Correct items collected"
x_label_ind <- "Target"
title_response_ind <- "Correct items collected (Individual) Vs. Target"
plot_name <- "InteractionPlot_CI_ByTarget_BySessionOrder_Ind.png"
setwd(figure_directory)

plot_data_ind <- ind_data %>%
  select(Target, SessionOrder, dependent_response_ind) %>%
  group_by(SessionOrder, Target) %>%
  summarise(Average = mean(.data[[dependent_response_ind]]), 
            Stdv =sd(.data[[dependent_response_ind]]), 
            n = length(.data[[dependent_response_ind]]), 
            StEr = sd(.data[[dependent_response_ind]]) / sqrt(length(.data[[dependent_response_ind]])))

ggplot(data = plot_data_ind, aes(x = Target, y = Average, color = SessionOrder, shape=SessionOrder)) +
  geom_point(size = 3) +
  geom_line(aes(group=SessionOrder, color = SessionOrder)) + 
  geom_errorbar(aes(ymin = Average - StEr, ymax = Average + StEr), width = 0.2) +
  labs(y = y_label_ind, x = x_label_ind, title = title_response_ind, fill = "Players") + 
  ggsave(filename = plot_name)

plot_data_ind %>%
  kable() %>%
  kable_styling()
```

Overall, there appears to be an interaction occurring between the Session Order and Target variable. In the Team condition, the number of correct items collected seems to improve over time. In the Ind_Team condition going from session 2 to session 3, the number of correct items decreases. In the Ind condition going from session 3 to 4, the number of correct items collected drops. 

__Reflection:__ There does seem to be interaction occurring in this plot, but it does not appear to be a strong interaction.

```{r}
dependent_response_ind <- "CI_ind"
y_label_ind <- "Correct items collected"
x_label_ind <- "Target"
title_response_ind <- "Correct items collected (Individual) Vs. Target"
plot_name <- "InteractionPlot_CI_BySessionOrder_ByTarget_Ind.png"
setwd(figure_directory)

plot_data_ind <- ind_data %>%
  select(Target, SessionOrder, dependent_response_ind) %>%
  group_by(SessionOrder, Target) %>%
  summarise(Average = mean(.data[[dependent_response_ind]]), 
            Stdv =sd(.data[[dependent_response_ind]]), 
            n = length(.data[[dependent_response_ind]]), 
            StEr = sd(.data[[dependent_response_ind]]) / sqrt(length(.data[[dependent_response_ind]])))

ggplot(data = plot_data_ind, aes(x = SessionOrder, y = Average, color = Target, shape=Target)) +
  geom_point(size = 3) +
  geom_line(aes(group=Target, color = Target)) + 
  geom_errorbar(aes(ymin = Average - StEr, ymax = Average + StEr), width = 0.2) +
  labs(y = y_label_ind, x = x_label_ind, title = title_response_ind, fill = "Players") +
  ggsave(filename = plot_name)
```

Overall, there seems to be some interaction with the data. 

In session 2, there seems to be little difference between the conditions.

In session 3, the Ind_Team condition is the lowest. The Team and Ind condition are similar. 

In session 4, the Team condition has the highest average correct items collected.  

__Reflection:__ There seems to be an interaction between the Target and the Session Order. There also seems to be a slight drop in correct items collected (i.e., performance) for the Ind_Team and Ind condition. The Team condition is the only condition that seems to steadly increas over time. This suggests that the Team condtion prodcued the most realiable amount of correct items collected at the individual level. 


#### Summary /Reflection

Overall, there some seems to be some interaction occuring within the data. It seems as though the team condition produces the highest value of correct items collected. The team condtion also appears to realiably increase over time, while other conditions seem to decrease a little between sessions. The next step is to determine if any of these interactions are statistically significant.

### Q: Is there an interaction between the Target and Session Order for the __correct items collected__ that is statistically significant?

I am interested in determining if a linear mixed effect model should include the interaction effect between the Target and Session Order variable.

There are a few steps I will take. First, I will fit the multiple models using the data provided. Second, I will compare those models to the null model. Third, I will pick the models that are worth exploring further (e.g., models that are significantly different from the null model). Fourth, I will evaluate the effect size ($R^2$) of the fixed effect and random effect variables using a method by [@Nakagawa2013, @Johnson2014]. Lastly, I will analyze the assumptions for the residuals. 

A note on the effect size ($R^2$). This method generates two types of ($R^2$) values called marginal ($R_{m}^2$) and conditional ($R_{c}^2$) effect size. The $R_{m}^2$ calculates how much of the variance is described by the fixed effect variables (feedback condition and session order) while the $R_{c}^2$ calculates how much of the variance is described by both the fixed and random effect variables.

#### Full Models Used for Team and Individual

The next two sections below describe the full model for team and individual level analysis. Five models were fitted and then compared. One model was the null model, one model was the full model, and the last three are a subset of the full model.

##### Team Full Model

$$ y_{ijt} = \mu + \alpha_{i} + \beta_{j} + \alpha_{i} \beta_{j} + \gamma_{t} +   \epsilon_{ijt}$$ 

Where $y_{ijt}$ is the response variable (e.g., team score), $\mu$ is the baseline (or intercept), $\alpha_{i}$ is the fixed effect for the $i^(th)$ feedback target category, $\beta_{j}$ is the fixed effect for the $j^(th)$ session order, $\gamma_{t}$ is the random effect for the $t^(th)$ team, $ \alpha_{i} \beta_{j}$ is the fixed effect of the interaction between Target and Session Order, and $\epsilon_{ijt}$ is the residual for the model. 

##### Individual Team Full Model

$$ y_{ijtp} = \mu + \alpha_{i} + \beta_{j} + \alpha_{i} \beta_{j} + \gamma_{t} + \theta_{p} + \epsilon_{ijtp}$$ 

Where $y_{ijtp}$ is the response variable (e.g., individual score), $\mu$ is the baseline (or intercept), $\alpha_{i}$ is the fixed effect for the $i^(th)$ feedback target category, $\beta_{j}$ is the fixed effect for the $j^(th)$ session order, $\gamma_{t}$ is the random effect for the $t^(th)$ team, $\theta_{p}$ is the random effect for the $p^(th)$ individual, $ \alpha_{i} \beta_{j}$ is the fixed effect of the interaction between Target and Session Order, and $\epsilon_{ijtp}$ is the residual for the model.

##### Team

```{r message=FALSE}
# Response variable
response_variable <- "CI_team"

# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         response_variable,
         Target, 
         SessionOrder, 
         Team)

# Generate models with variable response
data_focus_team <- data_modified_team
model.null <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "null", is.team = TRUE, is.robust =FALSE)
model.All <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "All", is.team = TRUE, is.robust = FALSE)
model.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = FALSE)
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = TRUE, is.robust = FALSE)
model.NoInteraction.NoSession <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoSession", is.team = TRUE, is.robust = FALSE)

# There is an option to compare all the models with the anova(). However, the function does not compare all models to model.null
anova(model.null,
      model.All) %>%
  kable(caption = "ANOVA: Null and All") %>%
  kable_styling()

anova(model.null,
      model.NoInteraction) %>%
  kable(caption = "ANOVA: Null and No.Interaction") %>%
  kable_styling()

anova(model.null,
      model.NoInteraction.NoTarget) %>%
  kable(caption = "ANOVA: Null and No.Interaction.No.Target") %>%
  kable_styling()

anova(model.null,
      model.NoInteraction.NoSession) %>%
  kable(caption = "ANOVA: Null and No.Interaction.No.Session") %>%
  kable_styling()

```

The folowing models are significantly different from the null model: model.NoInteraction, model. NoInteraction.NoTarget.

model.NoInteraction: Session order and target are the fixed effects in this model.

model.NoInteraction.NoTarget: Session order is the only fixed effect in this model.

I noticed that both models do not include the Interaction betweent the terms.

I want to compare the models that are significanntly different from the null model (i.e., the models mentioned above) and compare them with one another. The goal is to see if the extra terms improve the model. 

```{r}
# Response variable
response_variable <- "CI_team"

# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         response_variable,
         Target, 
         SessionOrder, 
         Team)

# Generate models with variable response
data_focus_team <- data_modified_team

model.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = FALSE)
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = TRUE, is.robust = FALSE)

# Compare all of the models
anova(model.NoInteraction,
      model.NoInteraction.NoTarget) %>%
  kable(caption = "ANOVA: model.NoInteraction and model.NoInteraction.NoTarget") %>%
  kable_styling()
```

The results show that the model.NoInteraction model was not significantly better at describing the data than the model.NoIteraction.NoTarget. In other words, the results show that the model.NoInteraction is not significantly different from model.NoInteraction.NoTarget. This lack of significance suggests that the model.NoInteraction.NoTarget describes the data best. I noticed there is a small difference in the AIC (`r anova(model.NoInteraction, model.NoInteraction.NoTarget)["model.NoInteraction","AIC"] - anova(model.NoInteraction, model.NoInteraction.NoTarget)["model.NoInteraction.NoTarget","AIC"]`) and the BIC (`r anova(model.NoInteraction, model.NoInteraction.NoTarget)["model.NoInteraction","BIC"] - anova(model.NoInteraction, model.NoInteraction.NoTarget)["model.NoInteraction.NoTarget","BIC"]`) criteria values. Due to the small AIC and BIC difference between the two models, I cannot confidently say one model is clearly better than the other model, though the model.NoInteraction.NoTarget is a slightly better model.

I want to calculate the effect size of both models to see if there is a noticeable difference between the effect sizes.

```{r}
# Response variable
response_variable <- "CI_team"

# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         response_variable,
         Target, 
         SessionOrder, 
         Team)

# Generate models with variable response
data_focus_team <- data_modified_team

model.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = FALSE)
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = TRUE, is.robust = FALSE)

# Effect size (R^2)

r.squaredGLMM(model.NoInteraction.NoTarget) %>%
  kable(caption = "Table: Effect size for *No Interaction and No Target*") %>%
   kable_styling(full_width = F)

r.squaredGLMM(model.NoInteraction) %>%
  kable(caption = "Table: Effect size for *No Interaction*") %>%
   kable_styling(full_width = F)
```

There is little difference between the effect sizes for both models. The results show that model.NoInteraction has a slightly better effect size, but the difference is not noticeably different. 

__Reflection:__ Overall, the data suggests no strong interaction between Target and Session Order variables. Two models were significantly different from the model.null: __model.NoInteraction__ and __model.NoInteraction.NoTarget__. In the __model.NoInteraction__, the Target and Session Order variable are present. In the __model.NoInteraction.NoTarget__, only the Session Order variable is present. This suggests that the session order variable is the best predictor of the data. However, after more examination of both models (i.e., __model.NoInteraction.NoTarget__ and __model.NoInteraction__), I was not confident in ignoring the __model.NoInteraction__ for two reasons. First, there was a small difference between the AIC and BIC values. The "better" model has a lower BIC or AIC value. The __model.NoInteraction.NoTarget__ had a slightly better BIC and AIC value, meaning that the model is "better" than the __model.NoInteraction.NoTarget__. Second, the effect sizes for both models were very similar. The similar effect sizes indicate that the models have a similar capability in describing the data. 

As a result, I cannot confidently ignore __model.NoInteraction__ (the model I am most interested in). Therefore, I will include model.NoInteraction.NoTarget in further analysis.


##### Individual

```{r}
# Response Variable

response_variable <- "CI_ind"

# Data to model
data_modified_ind <- ind_data %>%
  select(response_variable,
         Target, 
         SessionOrder, 
         Team, 
         Player_ID)

data_focus_ind <- data_modified_ind
model.null <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "null", is.team = FALSE, is.robust = FALSE)
model.All <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "All", is.team = FALSE, is.robust = FALSE)
model.NoInteraction <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction", is.team = FALSE, is.robust = FALSE)
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = FALSE, is.robust = FALSE)
model.NoInteraction.NoSession <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction_NoSession", is.team = FALSE, is.robust = FALSE)

# There is an option to compare all the models with the anova(). However, the function does not compare all models to model.null
anova(model.null,
      model.All) %>%
  kable(caption = "ANOVA: model.null and model.All") %>%
  kable_styling()

anova(model.null,
      model.NoInteraction) %>%
  kable(caption = "ANOVA: model.null and model.NoInteraction") %>%
  kable_styling()

anova(model.null,
      model.NoInteraction.NoTarget) %>%
  kable(caption = "ANOVA: model.null and model.NoInteraction.NoTarget") %>%
  kable_styling()

anova(model.null,
      model.NoInteraction.NoSession) %>%
  kable(caption = "ANOVA: model.null and model.NoInteraction.NoSession") %>%
  kable_styling()

```

The following models were significantly better than the null model: model.All, model.NoInteraction, and model.NoInteraction.NoTarget.

model.All: Session Order, Target, and the interaction are included in this model.

model.NoInteraction: Session order and target are the fixed effects in this model.

model.NoInteraction.NoTarget: Session order is the only fixed effect in this model.

```{r}
# Response Variable

response_variable <- "CI_ind"

# Data to model
data_modified_ind <- ind_data %>%
  select(response_variable,
         Target, 
         SessionOrder, 
         Team, 
         Player_ID)

data_focus_ind <- data_modified_ind
model.All <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "All", is.team = FALSE, is.robust = FALSE)
model.NoInteraction <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction", is.team = FALSE, is.robust = FALSE)
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = FALSE, is.robust = FALSE)

anova(model.All,
      model.NoInteraction) %>%
  kable(caption = "ANOVA: All and No.Interaction") %>%
  kable_styling()

anova(model.All,
      model.NoInteraction.NoTarget) %>%
  kable(caption = "ANOVA: All and NoInteraction.NoTarget") %>%
  kable_styling()

anova(model.NoInteraction,
      model.NoInteraction.NoTarget) %>%
  kable(caption = "ANOVA: No.Interaction and NoInteraction.NoTarget") %>%
  kable_styling()
```

The restuls show that the __model.All__ was not significanntly different from the __model.NoInteraction__. The AIC values are similar but BIC values are noticably different. This specific results suggests that we should use the simliar model, which in this case is the __model.NoInteraction__. However, __model.NoInteraction__ is not significantly different from __model.NoInteraction.NoTarget__ but is it approaching significence (p = `r view(anova(model.NoInteraction, model.NoInteraction.NoTarget))['model.NoInteraction','Pr(>Chisq)']`), which suggest that more data is needed to reach significance. I also notice that the model with the lowest AIC or BIC value is not consistant. The __model.NoInteraction__ has the lowest AIC value and __model.NoInteraction.NoTarget__ has the lowest BIC value. 

I want to look at the effect size of each model (i.e., __model.NoInteraction__ and __model.NoInteraction.NoTarget__) see if one model has a noticeably larger effect size when compared with the other model.

```{r}
# Response Variable

response_variable <- "timeRemaining_ind"

# Data to model
data_modified_ind <- ind_data %>%
  select(response_variable,
         Target, 
         SessionOrder, 
         Team, 
         Player_ID)

data_focus_ind <- data_modified_ind
model.NoInteraction <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction", is.team = FALSE, is.robust = FALSE)
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = FALSE, is.robust = FALSE)

# Effect size (R^2)

r.squaredGLMM(model.NoInteraction) %>%
  kable(caption = "Table: Effect size for *No Interaction*") %>%
   kable_styling(full_width = F)

r.squaredGLMM(model.NoInteraction.NoTarget) %>%
  kable(caption = "Table: Effect size for *No Interaction and No Target*") %>%
   kable_styling(full_width = F)
```

__Reflection:__ Overall, suggest that __model.NoInteraction__ and __model.NoInteraction.NoTarget__ are the best models to describe the data. However, the data is not clear on which model is better at describing the data. As a result, I will include __model.NoInteraction__ and __model.NoInteraction.NoTarget__ in further analysis. 


##### Summary / Reflection

Overall, none of the models that described the data best included the interaction term. None of the models that were better at describing the data included the interaction term. At the team and individual level, two models did the best job of describing the data: __model.NoInteraction.NoTarget__ and __model.NoInteraction__. At the team level, the __model.NoInteraction.NoTarget__ had a slight advantage over __model.NoInteraction__ but ultimately, I was not confident in ignoring either model. At the individual level, neither model seemed to have an advantage over the other model. In further analysis, I will include __model.NoInteraction.NoTarget__ and __model.NoInteraction__. 

My next step is to examine the models and see what I can learn from these models. Are any of the effects in these models significantly different from zero?

### Q: Are any of the effects in the models statistically different from zero?

The purpose of this question is to see what we can learn from these models. Before I examine the model by generating diagnostic plots for the models. The assumptions are as follows [@Galwey2014]:

1. The residuals should have an approximately normal distribution (i.e., a bell curve) when plotted on a histogram.
2. A fitted-value plot should show an almost constant width when viewed from left to right.
3. The points on a normal plot should lie on an almost straight line from the bottom left to the top right.

The models may violate some of the assumptions. According to @Field2017, the best way to examine the influence of assumption violations is to compare a robust model to a standard model. When models violate assumptions, I will compare a robust model to a classic model. If there is a noticeable difference between the two models (i.e., if the estimated coefficient are noticeably different) then I will use a step on the power ladder to transform (@Tukey1977), or re-express, the data to address the assumption violation.

#### Team

We learned that __model.NoInteraction.NoTarget__ and __model.NoInteraction__ best describe the team level data at the team level (see previous analysis). The model __model.NoInteraction__ is the model of interest but I cannot ignore __model.NoInteraction.NoTarget__.

```{r}
# Response variable
response_variable <- "CI_team"
plot_dependent_variable_name <- "CI"

# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         response_variable,
         Target, 
         SessionOrder, 
         Team)

# Plot location
setwd(figure_directory)

# Generate models with variable response
data_focus_team <- data_modified_team
model.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = FALSE)

# Histogram of Residuals - model.NoInteraction
ggplot(model.NoInteraction, aes(x = .resid)) +
  geom_histogram(bins = 30) + 
  labs(title = paste("Histogram of Residuals for model.NoInteraction"), x = "", y = "") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggsave(filename = paste("Histogram_Residuals_", plot_dependent_variable_name, "_NoInteraction_Team.png", sep = ""))

# Fitted values - model.NoInteraction
ggplot(model.NoInteraction, aes(x = .fitted, y = .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0)+
  labs(title = paste("Fitted-Value Plot for model.NoInteraction"), x = "Fitted Values", y = "Residuals") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggsave(filename = paste("FitteValue_Residuals_", plot_dependent_variable_name, "_NoInteraction_Team.png", sep = ""))

# QQ plots - model.NoInteraction
ggplot(model.NoInteraction, aes(sample = .resid)) +
  stat_qq() +
  stat_qq_line()+
  labs(title = "Normal Q-Q Plot for NoInteraction", x = "Theoretical", y = "Sample") +
   theme(plot.title = element_text(hjust = 0.5)) +
  ggsave(filename = paste("QQ_Residuals_", plot_dependent_variable_name, "_NoInteraction_Team.png", sep = ""))
```

__model.NoInteraction__: Visually, it looks like assumptions 2 and 3 are violated in the fitted-value plot and the QQ plot, respectively. I will need to compare this model to a robust model to examine the influence of the apparent violation. If there is a noticeable difference, I will need to transform the data.

```{r}
# Response variable 
response_variable <- "CI_team"

# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         response_variable,
         Target, 
         SessionOrder, 
         Team)

# Plot location
setwd(figure_directory)

# Generate models with variable response

data_focus_team <- data_modified_team
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = TRUE, is.robust = FALSE)

# Histogram of Residuals - model.NoInteraction.NoTarget
ggplot(model.NoInteraction.NoTarget, aes(x = .resid)) +
  geom_histogram(bins = 30) + 
  labs(title = paste("Histogram of Residuals for model.NoInteraction.NoTarget"), x = "", y = "") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggsave(filename = paste("Histogram_Residuals_", plot_dependent_variable_name, "_NoInteraction.NoTarget_Team.png", sep = ""))

# Fitted values - model.NoInteraction.NoTarget
ggplot(model.NoInteraction.NoTarget, aes(x = .fitted, y = .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0)+
  labs(title = paste("Fitted-Value Plot for model.NoInteraction.NoTarget"), x = "Fitted Values", y = "Residuals") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggsave(filename = paste("FitteValue_Residuals_", plot_dependent_variable_name, "_NoInteraction.NoTarget_Team.png", sep = ""))

# QQ plots - model.NoInteraction.NoTarget
ggplot(model.NoInteraction.NoTarget, aes(sample = .resid)) +
  stat_qq() +
  stat_qq_line()+
  labs(title = "Normal Q-Q Plot for NoInteraction.NoTarget", x = "Theoretical", y = "Sample") +
   theme(plot.title = element_text(hjust = 0.5)) +
  ggsave(filename = paste("QQ_Residuals_", plot_dependent_variable_name, "_NoInteraction.NoTarget_Team.png", sep = ""))
```

__model.NoInteraction.NoTarget__: Visually, it looks like assumptions 2 and 3 are violated in the fitted-value plot and the QQ plot, respectively. I will need to compare this model to a robust model to examine the influence of the apparent violation. If there is a noticeable difference, I will need to transform the data.

__Reflection:__ It looks both models violated assumptions 2 and 3. I need to examine how this visual violation influences the model. 

##### Q: Does the violation influence the linear model estimation?

I will use the robust estimation for linear mixed models found in the robustlmm package by @Koller2016. I will then compare the robust model to the non-robust model. If the estimated values are noticeably different, then the data will need to be transformed. 

```{r}
# Response Variable
response_variable <- "CI_team"

# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         response_variable,
         Target, 
         SessionOrder, 
         Team)

data_focus_team <- data_modified_team

# Generate robust and non-robust models - NoInteraction
model.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = FALSE)
rmodel.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = TRUE)

comparision_results <- compare(model.NoInteraction, rmodel.NoInteraction, show.rho.functions = FALSE)

comparision_results %>%
  kable(caption = "model.NoInteraction") %>%
  kable_styling()
```

I do not see a conerning difference between the robust and non-robust __model.NoInteraction__. This suggests that the data does not need to be transformed.

```{r}
# Response Variable
response_variable <- "CI_team"

# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         response_variable,
         Target, 
         SessionOrder, 
         Team)

data_focus_team <- data_modified_team

# Generate robust and non-robust models - NoInteraction.NoTarget
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = TRUE, is.robust = FALSE)
rmodel.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = TRUE, is.robust = TRUE)

comparision_results <- compare(model.NoInteraction.NoTarget, rmodel.NoInteraction.NoTarget, show.rho.functions = FALSE)

comparision_results %>%
  kable(caption = "model.NoInteraction.NoTarget") %>%
  kable_styling()
```

I do not see a conerning difference between the robust and non-robust __model.NoInteraction.NoTarget__. This suggests that the data does not need to be transformed.

##### Q: Are there any effects that are significant?

I want to determine if Target has a significant effect. I will examine the model itself, and then I will calculate the estimated marginal means to compare all three levels.

###### Model.NoInteraction
```{r}
# Response Variable
response_variable <- "CI_team"

# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         response_variable,
         Target, 
         SessionOrder, 
         Team)

data_focus_team <- data_modified_team

# Generate robust and non-robust models - NoInteraction
model.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = FALSE)

summary(model.NoInteraction)
```

The two variables are Target ans Session Order. The reference variable is session 2 (not session 1). The data suggest that the Team and Ind_Team level do not have a significant effect in reference to the Ind level. It is also intersting to note tht session 3 does not have a significant effect in reference to session 2. However, session 4 does have a significant effect in reference to session 2.

```{r}
# Response Variable
response_variable <- "CI_team"

# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         response_variable,
         Target, 
         SessionOrder, 
         Team) 

data_focus_team <- data_modified_team

# Generate robust and non-robust models - NoInteraction
model.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = FALSE)

emmeans(model.NoInteraction, list(pairwise ~ Target), adjust = "tukey")
```

The estimated margnial means shows no significant difference between the any of the conditions (I.e., Ind, Team, Ind_Team).

```{r}
# Response Variable
response_variable <- "CI_team"

# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         response_variable,
         Target, 
         SessionOrder, 
         Team)

data_focus_team <- data_modified_team

# Generate robust and non-robust models - NoInteraction
model.NoInteraction <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction", is.team = TRUE, is.robust = FALSE)

emmeans(model.NoInteraction, list(pairwise ~ SessionOrder), adjust = "tukey")
```

The results show only a significant difference between session 2 and session 4.

__Reflection:__ There is no significant effects from the Target variables. There is also, only a significant difference between sessions 2 and 4. This suugests that there is little difference between session 2 and 3. This suggstion would that teams did not increase the number of correct team items collected when going from session 2 to 3. However, the teams did get significantly better when comparing session 2 to session 4. This would mean that teams got better overall.

###### Model.NoInteraction.NoTarget

```{r}
# Response Variable
response_variable <- "CI_team"

# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         response_variable,
         Target, 
         SessionOrder, 
         Team)

data_focus_team <- data_modified_team

# Generate robust and non-robust models - NoInteraction_NoTarget
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = TRUE, is.robust = FALSE)

summary(model.NoInteraction.NoTarget)
```

The results suggests that effect of session 3 was not significantly differeny from session 2, though it is approaching significance. The results also suggest that the effect of session 4 is significantly different from session 2.

```{r}
# Response Variable
response_variable <- "CI_team"

# Data to fit
data_modified_team <- team_data %>%
  select(TeamScore,
         response_variable,
         Target, 
         SessionOrder, 
         Team)

data_focus_team <- data_modified_team

# Generate robust and non-robust models - NoInteraction_NoTarget
model.NoInteraction.NoTarget <- model_data_Target_Session(df = data_focus_team, dependent =  response_variable, model.type =  "NoInteraction_NoTarget", is.team = TRUE, is.robust = FALSE)

emmeans(model.NoInteraction.NoTarget, list(pairwise ~ SessionOrder), adjust = "tukey")
```

The results indiciate there is a significant difference between session 2 and 4. 

__Reflection:__ Overall, the only major finding is that there is only a significant difference between sesson 2 and session 4.

##### Summary / Reflection

Overall, neither model showed any unexpected or exciting significance. There was a lack of significance when examining the session order variable but I believe that that is due to the low number of teams. 


#### Individual

We learned that __model.NoInteraction__ best describe the individual level data(see previous analysis).
__[Stopping point!]__

```{r}
# Response variable
response_variable <- "CI_ind"
plot_dependent_variable_name <- "CI"

# Data to model
data_modified_ind <- ind_data %>%
  select(response_variable,
         Target, 
         SessionOrder, 
         Team, 
         Player_ID)

# Plot location
setwd(figure_directory)

# Generate models with variable response

data_focus_ind <- data_modified_ind
model.NoInteraction <- model_data_Target_Session(df = data_focus_ind, dependent =  response_variable, model.type =  "NoInteraction", is.team = FALSE, is.robust = FALSE)

# Histogram of Residuals - model.NoInteraction
ggplot(model.NoInteraction, aes(x = .resid)) +
  geom_histogram(bins = 30) + 
  labs(title = paste("Histogram of Residuals for model.NoInteraction"), x = "", y = "") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggsave(paste("Histogram_Residuals_", plot_dependent_variable_name, "_NoInteraction_Ind.png", sep =""))

# Fitted values - model.NoInteraction
ggplot(model.NoInteraction, aes(x = .fitted, y = .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0)+
  labs(title = paste("Fitted-Value Plot for model.NoInteraction"), x = "Fitted Values", y = "Residuals") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggsave(paste("FittedValue_Residuals_", plot_dependent_variable_name, "_NoInteraction_Ind.png", sep =""))


# QQ plots - model.NoInteraction
ggplot(model.NoInteraction, aes(sample = .resid)) +
  stat_qq() +
  stat_qq_line()+
  labs(title = "Normal Q-Q Plot for NoInteraction", x = "Theoretical", y = "Sample") +
   theme(plot.title = element_text(hjust = 0.5)) +
  ggsave(paste("QQ_Residuals_", plot_dependent_variable_name, "_NoInteraction_Ind.png", sep =""))
```


#### Summary and reflection


<!-- ### Q: Is there exciting variation among the *Collection_rate_correct_item* that will help me understand the influence of the feedback condition? -->

<!-- I want to examine the distribution of the collection rate for correct at the individual and team level.  -->

<!-- $$ Collection Rate for Correct Items (Team) = \frac{duration_{Team}}{total.correct.items.collected_{Team}}$$ -->

<!-- Where $duration_{Team}$ is the amount of time the group spent completing the task and $total.correct.items.collected_{Team}$ is the total number correct items (individual or team items) collected by the group.  -->

<!-- $$ Collection Rate for Correct Items (Individual) = \frac{duration_{Individual}}{total.correct.items.collected_{Individual}}$$ -->

<!-- Where $duration_{Individual}$ is the amount of time the individual spent completing the task and $total.correct.items.collected_{Individual}$ is the total number correct items (individual or team items) collected by the individual.  -->

<!-- I divided the duration by the number of items to collect because dividing the total correct items collected by the duration was hard to interpret.  -->

<!-- _Note_: It is important to remember that a high performing team would have a lower collection rate. Low values mean that they need less time to collect correct items. -->

<!-- #### Team -->

<!-- ```{r} -->
<!-- dependent_response_team <- "Collection_rate_correct_item_team" -->
<!-- y_label_team <- "Count" -->
<!-- x_label_team <- "Correct Item Collection Rate" -->
<!-- title_response_team <- "Distribution of Correct Item Collectione Rate (Team)" -->
<!-- plot_name <- "Histogram_CollectionRateCorrect_Team.png" -->

<!-- # Plot location -->
<!-- setwd(figure_directory) -->

<!-- plot_data_team <- team_data %>% -->
<!--   select(dependent_response_team) -->

<!-- ggplot(data = plot_data_team) +  -->
<!--   geom_histogram(aes_string(x = dependent_response_team), bins = 30) + -->
<!--   labs(title = title_response_team, x = x_label_team, y = y_label_team) + -->
<!--   ggsave(plot_name) -->
<!-- ``` -->

<!-- The distribution tells me that most groups had a correct item collection rate of about 18 sec per correct item. In other words, groups generally needed about 18 secs to collect a correct item. -->

<!-- ```{r} -->
<!-- dependent_response_team <- "Collection_rate_correct_item_team" -->
<!-- y_label_team <- "Count" -->
<!-- x_label_team <- "Correct Item Collection Rate" -->
<!-- title_response_team <- "Distribution of Correct Item Collectione Rate (Team)" -->

<!-- # Plot Location -->
<!-- setwd(figure_directory) -->

<!-- plot_data_team <- team_data %>% -->
<!--   select(dependent_response_team, Target) -->

<!-- ggplot(data = plot_data_team) +  -->
<!--   geom_histogram(aes_string(x = dependent_response_team), bins = 30) + -->
<!--   facet_grid(. ~ Target) + -->
<!--   labs(title = title_response_team, x = x_label_team, y = y_label_team) +  -->
<!--   ggsave("Histogram_CollectionRateCorrect_ByTarget_Team.png") -->

<!-- # Information about the N values in each condition -->
<!-- tableData <- team_data %>% -->
<!--   group_by(Target) %>% -->
<!--   summarise(N = length(.data[[dependent_response_team]])) -->

<!-- tableData %>% -->
<!--   kable() %>% -->
<!--   kable_styling() -->
<!-- ``` -->

<!-- The plot tells me that the distribution in the feedback conditions is similar to the distribution overall. However, it does look like the correct item collection rate is slightly higher in the Team condition when compared to the Ind condition. The slight increase in the center (i.e., average) tells me that the groups needed more time to collect the correct items in the Team condition. -->

<!-- #### Individual -->

<!-- ```{r} -->
<!-- dependent_response_ind <- "Collection_rate_correct_item_ind" -->
<!-- y_label_ind <- "Count" -->
<!-- x_label_ind <- "Correct Item Collection Rate" -->
<!-- title_response_ind <- "Distribution of Correct Item Collectione Rate (Individual)" -->

<!-- # Plot Location -->
<!-- setwd(figure_directory) -->

<!-- plot_data_ind <- ind_data %>% -->
<!--   select(dependent_response_ind) -->

<!-- ggplot(data = plot_data_ind) +  -->
<!--   geom_histogram(aes_string(x = dependent_response_ind), bins = 30) + -->
<!--   labs(title = title_response_ind, x = x_label_ind, y = y_label_ind) + -->
<!--   ggsave("Histogram_CollectionRateCorrect_Ind.png") -->
<!-- ``` -->

<!-- The distribution plot tells me that the collection rate for most individuals is around 50 second per correct item collected. This collection rate is higher than the collection rate of the team as a whole. This higher collection rate tells me that individuals needed more time to collect correct items. The collection rate at the individual level is higher than the team level makes sense because at the team level there are more people collecting items. -->

<!-- ```{r} -->
<!-- dependent_response_ind <- "Collection_rate_correct_item_ind" -->
<!-- y_label_ind <- "Count" -->
<!-- x_label_ind <- "Correct Item Collection Rate" -->
<!-- title_response_ind <- "Distribution of Correct Item Collectione Rate (Individual)" -->

<!-- # Plot Location -->
<!-- setwd(figure_directory) -->

<!-- plot_data_ind <- ind_data %>% -->
<!--   select(Target, dependent_response_ind) -->

<!-- ggplot(data = plot_data_ind) +  -->
<!--   geom_histogram(aes_string(x = dependent_response_ind), bins = 30) + -->
<!--   facet_grid(. ~ Target) +  -->
<!--   labs(title = title_response_ind, x = x_label_ind, y = y_label_ind) + -->
<!--   ggsave("Histogram_CollectionRateCorrect_ByTarget_Ind.png") -->

<!-- # Information about the N values in each condition -->
<!-- tableData <- ind_data %>% -->
<!--   group_by(Target) %>% -->
<!--   summarise(N = length(.data[[dependent_response_team]])) -->

<!-- tableData %>% -->
<!--   kable() %>% -->
<!--   kable_styling() -->
<!-- ``` -->

<!-- The distribution plot is similar to the overall distribution plot. However, the majority of participants in the Team condition seem to need around 50 seconds to collect a correct item while participants in the Ind condition required 60 or 70 seconds. -->

<!-- #### Summary / Reflection -->

<!-- At the team level, most groups needed about 20 seconds on average to collect a correct team item. The center of the distributions, when separated by Target, are roughly similar. The centers were also hovering around 20 for each distribution. -->

<!-- At the individual level, most participants needed about 50 seconds to collect a correct item. When I group the data by Target levels, the distribution is similar to the overall distribution.  -->

<!-- How does the session order influence the collection rate distribution? -->

<!-- The next step is to see how the distributions look when I group the data by Target and Session Order.  -->



<!-- ### Q: Is there exciting variation among the *Collection_rate_correct_item* that explains how the feedback conditions influence the collection rate of correct items when session order is taken into consideration? -->

<!-- #### Team -->
<!-- ```{r} -->
<!-- dependent_response_team <- "Collection_rate_correct_item_ind" -->
<!-- y_label_team <- "Count" -->
<!-- x_label_team <- "Correct Item Collection Rate" -->
<!-- title_response_team <- "Distribution of Correct Item Collectione Rate (Team)" -->
<!-- plot_name <- "Histogram_CollectionRateCorrect_ByTarget_BySessionOrder_Team.png" -->
<!-- setwd(figure_directory) -->

<!-- plot_data_team <- team_data %>% -->
<!--   select(dependent_response_team, Target, SessionOrder) -->

<!-- ggplot(data = plot_data_team) + -->
<!--   geom_histogram(aes_string(x = dependent_response_team), bins = 30) + -->
<!--   labs(title = title_response_team, x = x_label_team, y = y_label_team) + -->
<!--   facet_grid( SessionOrder ~ Target) + -->
<!--   ggsave(filename = plot_name) -->

<!-- # Information about the N values in each condition -->
<!-- tableData <- team_data %>% -->
<!--   select(dependent_response_team, Target, SessionOrder) %>% -->
<!--   group_by(Target, SessionOrder) %>% -->
<!--   summarise(N = length(.data[[dependent_response_team]])) -->

<!-- tableData %>% -->
<!--   kable() %>% -->
<!--   kable_styling() -->
<!-- ``` -->

<!-- Overall, there seems to be no pattern with the distrabution of the data over time. For each condition, it looks as though teams needed less and less time to collect correct items. It looks as though the biggest imporvement (i.e., comparing the 2nd session to the 4th session) occurs in the team condition. Groups in this condition seem to have the biggest imporvement from session 2 to session 4. There are improvements in the Ind and Ind_Team condition but the most obvious improvement is in the Team condition. -->

<!-- _Reflection:_ When veiwing this chart I expected to see improvement over time. In other words, I expected to see a shift in overall distrabution to the left. Each condition had some amount of shift but the most obvious shift comes from Team condtion. -->

<!-- #### Individual -->

<!-- #### Summary / Reflection -->





# References
